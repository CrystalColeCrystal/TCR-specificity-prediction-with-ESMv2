{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1c29b3f-8551-43d8-8c7a-2325db955bd5",
   "metadata": {},
   "source": [
    "# This notebook demonstrates a training example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63772bb7-c689-4f20-b040-34ddae437bb2",
   "metadata": {},
   "source": [
    "### 1. To run it please create a conda environment called training_env using main/requirments_training_env.txt and use it for trainig in this notebook of from command line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b6b1484-2aa3-4e14-804e-5383ce092f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! conda activate training env"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "581b0f70-a1e1-4dda-b035-be5ef4a4e631",
   "metadata": {},
   "source": [
    "### 2. Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad2a1772-0e1f-412e-bebd-ab7f6f128478",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mkoshkina/miniconda3/envs/ptch/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import evaluate\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import Dataset\n",
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "\n",
    "import comet_ml\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b52562d1-e61d-4e80-b361-22ec5c692b33",
   "metadata": {},
   "source": [
    "### 3. Check GPU availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7e4bc993-fa5c-4f48-a87e-84bc3af51ef2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tesla V100S-PCIE-32GB'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "203dafc5-0ef2-4c78-bea6-1971dc236574",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e258954-a5f7-4308-bac9-785626de27c8",
   "metadata": {},
   "source": [
    "### 4. Initialize comet-ml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae399f86-625f-4faf-b4b0-417a6e4b5392",
   "metadata": {},
   "source": [
    "comet_ml.init(project_name='esm2_t6_8M_transfer_learning', api_key='INSERT YOUR COMET ML API KEY HERE', experiment_name='experiment_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c82324fc-6ba4-41a3-a8b8-bf81fab33ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"COMET_LOG_ASSETS\"] = \"True\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2bcfbd7-0f87-49e0-94ba-b84496fe65f7",
   "metadata": {},
   "source": [
    "### 5. Load and prepare the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee90dd80-186b-4aba-9110-400b2480977e",
   "metadata": {},
   "source": [
    "The training and validation data can be found in the main/data folder of the current repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e3ede322-6eda-4d37-9153-0cb96f7f0e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('~/TCR-specificity-prediction-with-ESMv2/data/train.csv')\n",
    "hard_test = pd.read_csv('~/TCR-specificity-prediction-with-ESMv2/data/hard_test.csv')\n",
    "easy_test = pd.read_csv('~/TCR-specificity-prediction-with-ESMv2/data/easy_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d374973f-0b70-4d19-9621-0e4a4bffb97f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sequences = list(train['cdr3.alpha']+train['cdr3.beta']+train['antigen.epitope'])\n",
    "hard_test_sequences = list(hard_test['cdr3.alpha']+hard_test['cdr3.beta']+hard_test['antigen.epitope'])\n",
    "easy_test_sequences = list(easy_test['cdr3.alpha']+easy_test['cdr3.beta']+easy_test['antigen.epitope'])\n",
    "train_labels = list(train['target'])\n",
    "hard_test_labels = list(hard_test['target'])\n",
    "easy_test_labels = list(easy_test['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d05ab08f-5c2d-46a1-82a7-744ec6c333f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d9608113-cfec-483b-a93e-fb5b8e343e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokenized = tokenizer(train_sequences)\n",
    "hard_test_tokenized = tokenizer(hard_test_sequences)\n",
    "easy_test_tokenized = tokenizer(easy_test_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "50b87b13-db95-48f3-be1b-9855805a9b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Dataset.from_dict(train_tokenized)\n",
    "hard_test_dataset = Dataset.from_dict(hard_test_tokenized)\n",
    "easy_test_dataset = Dataset.from_dict(easy_test_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ecdda6c1-c7be-46a2-a2b3-c170c3bb79f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 49014\n",
       "})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = train_dataset.add_column(\"labels\", train_labels)\n",
    "hard_test_dataset = hard_test_dataset.add_column(\"labels\", hard_test_labels)\n",
    "easy_test_dataset = easy_test_dataset.add_column(\"labels\", easy_test_labels)\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c825a12f-60b2-4146-99a2-c14567ffdd1b",
   "metadata": {},
   "source": [
    "### 6. Load the esm2_t6_8M model with a classification layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a261c11e-b63c-427e-ab88-79e1a0d45a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = \"facebook/esm2_t6_8M_UR50D\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a945d73f-aa72-440a-81ce-95d6ea563ad8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmForSequenceClassification were not initialized from the model checkpoint at facebook/esm2_t6_8M_UR50D and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c4eaf84-6193-4080-88be-183816f81d10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EsmForSequenceClassification(\n",
      "  (esm): EsmModel(\n",
      "    (embeddings): EsmEmbeddings(\n",
      "      (word_embeddings): Embedding(33, 320, padding_idx=1)\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "      (position_embeddings): Embedding(1026, 320, padding_idx=1)\n",
      "    )\n",
      "    (encoder): EsmEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-5): 6 x EsmLayer(\n",
      "          (attention): EsmAttention(\n",
      "            (self): EsmSelfAttention(\n",
      "              (query): Linear(in_features=320, out_features=320, bias=True)\n",
      "              (key): Linear(in_features=320, out_features=320, bias=True)\n",
      "              (value): Linear(in_features=320, out_features=320, bias=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "              (rotary_embeddings): RotaryEmbedding()\n",
      "            )\n",
      "            (output): EsmSelfOutput(\n",
      "              (dense): Linear(in_features=320, out_features=320, bias=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (LayerNorm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (intermediate): EsmIntermediate(\n",
      "            (dense): Linear(in_features=320, out_features=1280, bias=True)\n",
      "          )\n",
      "          (output): EsmOutput(\n",
      "            (dense): Linear(in_features=1280, out_features=320, bias=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (LayerNorm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (emb_layer_norm_after): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (contact_head): EsmContactPredictionHead(\n",
      "      (regression): Linear(in_features=120, out_features=1, bias=True)\n",
      "      (activation): Sigmoid()\n",
      "    )\n",
      "  )\n",
      "  (classifier): EsmClassificationHead(\n",
      "    (dense): Linear(in_features=320, out_features=320, bias=True)\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "    (out_proj): Linear(in_features=320, out_features=2, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ce497b-c734-4d26-b3ae-c365d08641a8",
   "metadata": {},
   "source": [
    "Let's look if the model's original layers are frozen or not:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "04dc4606-7b5d-4afd-a3b8-1f39fb031d16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "esm.embeddings.word_embeddings.weight: True\n",
      "esm.embeddings.position_embeddings.weight: True\n",
      "esm.encoder.layer.0.attention.self.query.weight: True\n",
      "esm.encoder.layer.0.attention.self.query.bias: True\n",
      "esm.encoder.layer.0.attention.self.key.weight: True\n",
      "esm.encoder.layer.0.attention.self.key.bias: True\n",
      "esm.encoder.layer.0.attention.self.value.weight: True\n",
      "esm.encoder.layer.0.attention.self.value.bias: True\n",
      "esm.encoder.layer.0.attention.output.dense.weight: True\n",
      "esm.encoder.layer.0.attention.output.dense.bias: True\n",
      "esm.encoder.layer.0.attention.LayerNorm.weight: True\n",
      "esm.encoder.layer.0.attention.LayerNorm.bias: True\n",
      "esm.encoder.layer.0.intermediate.dense.weight: True\n",
      "esm.encoder.layer.0.intermediate.dense.bias: True\n",
      "esm.encoder.layer.0.output.dense.weight: True\n",
      "esm.encoder.layer.0.output.dense.bias: True\n",
      "esm.encoder.layer.0.LayerNorm.weight: True\n",
      "esm.encoder.layer.0.LayerNorm.bias: True\n",
      "esm.encoder.layer.1.attention.self.query.weight: True\n",
      "esm.encoder.layer.1.attention.self.query.bias: True\n",
      "esm.encoder.layer.1.attention.self.key.weight: True\n",
      "esm.encoder.layer.1.attention.self.key.bias: True\n",
      "esm.encoder.layer.1.attention.self.value.weight: True\n",
      "esm.encoder.layer.1.attention.self.value.bias: True\n",
      "esm.encoder.layer.1.attention.output.dense.weight: True\n",
      "esm.encoder.layer.1.attention.output.dense.bias: True\n",
      "esm.encoder.layer.1.attention.LayerNorm.weight: True\n",
      "esm.encoder.layer.1.attention.LayerNorm.bias: True\n",
      "esm.encoder.layer.1.intermediate.dense.weight: True\n",
      "esm.encoder.layer.1.intermediate.dense.bias: True\n",
      "esm.encoder.layer.1.output.dense.weight: True\n",
      "esm.encoder.layer.1.output.dense.bias: True\n",
      "esm.encoder.layer.1.LayerNorm.weight: True\n",
      "esm.encoder.layer.1.LayerNorm.bias: True\n",
      "esm.encoder.layer.2.attention.self.query.weight: True\n",
      "esm.encoder.layer.2.attention.self.query.bias: True\n",
      "esm.encoder.layer.2.attention.self.key.weight: True\n",
      "esm.encoder.layer.2.attention.self.key.bias: True\n",
      "esm.encoder.layer.2.attention.self.value.weight: True\n",
      "esm.encoder.layer.2.attention.self.value.bias: True\n",
      "esm.encoder.layer.2.attention.output.dense.weight: True\n",
      "esm.encoder.layer.2.attention.output.dense.bias: True\n",
      "esm.encoder.layer.2.attention.LayerNorm.weight: True\n",
      "esm.encoder.layer.2.attention.LayerNorm.bias: True\n",
      "esm.encoder.layer.2.intermediate.dense.weight: True\n",
      "esm.encoder.layer.2.intermediate.dense.bias: True\n",
      "esm.encoder.layer.2.output.dense.weight: True\n",
      "esm.encoder.layer.2.output.dense.bias: True\n",
      "esm.encoder.layer.2.LayerNorm.weight: True\n",
      "esm.encoder.layer.2.LayerNorm.bias: True\n",
      "esm.encoder.layer.3.attention.self.query.weight: True\n",
      "esm.encoder.layer.3.attention.self.query.bias: True\n",
      "esm.encoder.layer.3.attention.self.key.weight: True\n",
      "esm.encoder.layer.3.attention.self.key.bias: True\n",
      "esm.encoder.layer.3.attention.self.value.weight: True\n",
      "esm.encoder.layer.3.attention.self.value.bias: True\n",
      "esm.encoder.layer.3.attention.output.dense.weight: True\n",
      "esm.encoder.layer.3.attention.output.dense.bias: True\n",
      "esm.encoder.layer.3.attention.LayerNorm.weight: True\n",
      "esm.encoder.layer.3.attention.LayerNorm.bias: True\n",
      "esm.encoder.layer.3.intermediate.dense.weight: True\n",
      "esm.encoder.layer.3.intermediate.dense.bias: True\n",
      "esm.encoder.layer.3.output.dense.weight: True\n",
      "esm.encoder.layer.3.output.dense.bias: True\n",
      "esm.encoder.layer.3.LayerNorm.weight: True\n",
      "esm.encoder.layer.3.LayerNorm.bias: True\n",
      "esm.encoder.layer.4.attention.self.query.weight: True\n",
      "esm.encoder.layer.4.attention.self.query.bias: True\n",
      "esm.encoder.layer.4.attention.self.key.weight: True\n",
      "esm.encoder.layer.4.attention.self.key.bias: True\n",
      "esm.encoder.layer.4.attention.self.value.weight: True\n",
      "esm.encoder.layer.4.attention.self.value.bias: True\n",
      "esm.encoder.layer.4.attention.output.dense.weight: True\n",
      "esm.encoder.layer.4.attention.output.dense.bias: True\n",
      "esm.encoder.layer.4.attention.LayerNorm.weight: True\n",
      "esm.encoder.layer.4.attention.LayerNorm.bias: True\n",
      "esm.encoder.layer.4.intermediate.dense.weight: True\n",
      "esm.encoder.layer.4.intermediate.dense.bias: True\n",
      "esm.encoder.layer.4.output.dense.weight: True\n",
      "esm.encoder.layer.4.output.dense.bias: True\n",
      "esm.encoder.layer.4.LayerNorm.weight: True\n",
      "esm.encoder.layer.4.LayerNorm.bias: True\n",
      "esm.encoder.layer.5.attention.self.query.weight: True\n",
      "esm.encoder.layer.5.attention.self.query.bias: True\n",
      "esm.encoder.layer.5.attention.self.key.weight: True\n",
      "esm.encoder.layer.5.attention.self.key.bias: True\n",
      "esm.encoder.layer.5.attention.self.value.weight: True\n",
      "esm.encoder.layer.5.attention.self.value.bias: True\n",
      "esm.encoder.layer.5.attention.output.dense.weight: True\n",
      "esm.encoder.layer.5.attention.output.dense.bias: True\n",
      "esm.encoder.layer.5.attention.LayerNorm.weight: True\n",
      "esm.encoder.layer.5.attention.LayerNorm.bias: True\n",
      "esm.encoder.layer.5.intermediate.dense.weight: True\n",
      "esm.encoder.layer.5.intermediate.dense.bias: True\n",
      "esm.encoder.layer.5.output.dense.weight: True\n",
      "esm.encoder.layer.5.output.dense.bias: True\n",
      "esm.encoder.layer.5.LayerNorm.weight: True\n",
      "esm.encoder.layer.5.LayerNorm.bias: True\n",
      "esm.encoder.emb_layer_norm_after.weight: True\n",
      "esm.encoder.emb_layer_norm_after.bias: True\n",
      "esm.contact_head.regression.weight: True\n",
      "esm.contact_head.regression.bias: True\n",
      "classifier.dense.weight: True\n",
      "classifier.dense.bias: True\n",
      "classifier.out_proj.weight: True\n",
      "classifier.out_proj.bias: True\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(f\"{name}: {param.requires_grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ead12c4-bc3c-4ab1-b10a-e4705763a3ed",
   "metadata": {},
   "source": [
    "### 7. Specify the training arguments and metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "dfd83b42-9409-4121-abcf-f9e84138d881",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mkoshkina/miniconda3/envs/ptch/lib/python3.12/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model_name = model_checkpoint.split(\"/\")[-1]\n",
    "batch_size = 2048\n",
    "\n",
    "args = TrainingArguments(\n",
    "    f\"{model_name}-batch-size-test-finetuned-localization\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    save_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=300,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    push_to_hub=False,\n",
    "    label_names=[\"labels\"],\n",
    "    report_to=[\"comet_ml\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f4f5c6b1-36fa-4aef-bcd6-6033eb84c8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_metrics = evaluate.combine([\"accuracy\", \"f1\", \"precision\", \"recall\"])\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return clf_metrics.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f7ee62-3303-45aa-999e-9e616c04b8ef",
   "metadata": {},
   "source": [
    "### 8. Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "85940177-e581-43a4-b483-c2ca0cc9066a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=easy_test_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "90ccb91e-d7aa-47aa-bbd7-e1f3be43ee28",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m To get all data logged automatically, import comet_ml before the following modules: torch, sklearn.\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m ---------------------------------------------------------------------------------------\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Comet.ml Experiment Summary\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m ---------------------------------------------------------------------------------------\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Data:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     display_summary_level : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     name                  : batch_size_test\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     url                   : https://www.comet.com/crystalcolecrystal/esm2-t6-8m-transfer-learning/6a41be53a4584d1c9f47bd79730f219e\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Others:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     Name : batch_size_test\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Parameters:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/_n_gpu                                  : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/_no_sync_in_gradient_accumulation       : True\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/_setup_devices                          : cuda:0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/accelerator_config                      : AcceleratorConfig(split_batches=False, dispatch_batches=None, even_batches=True, use_seedable_sampler=True, non_blocking=False, gradient_accumulation_kwargs=None)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/adafactor                               : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/adam_beta1                              : 0.9\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/adam_beta2                              : 0.999\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/adam_epsilon                            : 1e-08\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/auto_find_batch_size                    : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/batch_eval_metrics                      : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/bf16                                    : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/bf16_full_eval                          : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/data_seed                               : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/dataloader_drop_last                    : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/dataloader_num_workers                  : 0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/dataloader_persistent_workers           : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/dataloader_pin_memory                   : True\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/dataloader_prefetch_factor              : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/ddp_backend                             : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/ddp_broadcast_buffers                   : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/ddp_bucket_cap_mb                       : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/ddp_find_unused_parameters              : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/ddp_timeout                             : 1800\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/ddp_timeout_delta                       : 0:30:00\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/debug                                   : []\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/deepspeed                               : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/deepspeed_plugin                        : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/default_optim                           : adamw_torch\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/device                                  : cuda:0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/disable_tqdm                            : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/dispatch_batches                        : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/distributed_state                       : Distributed environment: DistributedType.NO\n",
      "Num processes: 1\n",
      "Process index: 0\n",
      "Local process index: 0\n",
      "Device: cuda\n",
      "\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/do_eval                                 : True\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/do_predict                              : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/do_train                                : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/eval_accumulation_steps                 : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/eval_batch_size                         : 3072\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/eval_delay                              : 0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/eval_do_concat_batches                  : True\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/eval_steps                              : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/eval_strategy                           : IntervalStrategy.EPOCH\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/evaluation_strategy                     : epoch\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/fp16                                    : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/fp16_backend                            : auto\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/fp16_full_eval                          : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/fp16_opt_level                          : O1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/framework                               : pt\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/fsdp                                    : []\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/fsdp_config                             : {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/fsdp_min_num_params                     : 0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/fsdp_transformer_layer_cls_to_wrap      : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/full_determinism                        : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/gradient_accumulation_steps             : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/gradient_checkpointing                  : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/gradient_checkpointing_kwargs           : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/greater_is_better                       : True\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/group_by_length                         : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/half_precision_backend                  : auto\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/hub_always_push                         : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/hub_model_id                            : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/hub_private_repo                        : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/hub_strategy                            : HubStrategy.EVERY_SAVE\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/hub_token                               : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/ignore_data_skip                        : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/include_inputs_for_metrics              : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/include_num_input_tokens_seen           : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/include_tokens_per_second               : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/jit_mode_eval                           : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/label_names                             : ['labels']\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/label_smoothing_factor                  : 0.0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/learning_rate                           : 2e-05\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/length_column_name                      : length\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/load_best_model_at_end                  : True\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/local_process_index                     : 0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/local_rank                              : 0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/log_level                               : passive\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/log_level_replica                       : warning\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/log_on_each_node                        : True\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/logging_dir                             : esm2_t6_8M_UR50D-batch-size-test-finetuned-localization/runs/Jun06_17-52-47_gpu-2-2-19\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/logging_first_step                      : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/logging_nan_inf_filter                  : True\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/logging_steps                           : 500\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/logging_strategy                        : IntervalStrategy.STEPS\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/lr_scheduler_kwargs                     : {}\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/lr_scheduler_type                       : SchedulerType.LINEAR\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/max_grad_norm                           : 1.0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/max_steps                               : -1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/metric_for_best_model                   : accuracy\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/mp_parameters                           : \n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/n_gpu                                   : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/neftune_noise_alpha                     : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/no_cuda                                 : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/num_train_epochs                        : 300\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/optim                                   : OptimizerNames.ADAMW_TORCH\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/optim_args                              : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/optim_target_modules                    : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/output_dir                              : esm2_t6_8M_UR50D-batch-size-test-finetuned-localization\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/overwrite_output_dir                    : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/parallel_mode                           : ParallelMode.NOT_PARALLEL\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/past_index                              : -1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/per_device_eval_batch_size              : 3072\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/per_device_train_batch_size             : 3072\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/per_gpu_eval_batch_size                 : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/per_gpu_train_batch_size                : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/place_model_on_device                   : True\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/prediction_loss_only                    : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/process_index                           : 0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/push_to_hub                             : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/push_to_hub_model_id                    : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/push_to_hub_organization                : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/push_to_hub_token                       : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/ray_scope                               : last\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/remove_unused_columns                   : True\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/report_to                               : ['comet_ml']\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/restore_callback_states_from_checkpoint : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/resume_from_checkpoint                  : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/run_name                                : esm2_t6_8M_UR50D-batch-size-test-finetuned-localization\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/save_on_each_node                       : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/save_only_model                         : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/save_safetensors                        : True\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/save_steps                              : 500\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/save_strategy                           : IntervalStrategy.EPOCH\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/save_total_limit                        : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/seed                                    : 42\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/should_log                              : True\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/should_save                             : True\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/skip_memory_metrics                     : True\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/split_batches                           : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/tf32                                    : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/torch_compile                           : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/torch_compile_backend                   : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/torch_compile_mode                      : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/torchdynamo                             : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/tpu_metrics_debug                       : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/tpu_num_cores                           : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/train_batch_size                        : 3072\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/use_cpu                                 : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/use_ipex                                : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/use_legacy_prediction_loop              : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/use_mps_device                          : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/warmup_ratio                            : 0.0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/warmup_steps                            : 0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/weight_decay                            : 0.01\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/world_size                              : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/_attn_implementation                  : eager\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/_attn_implementation_internal         : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/_auto_class                           : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/_commit_hash                          : c731040fcd8d73dceaa04b0a8e6329b345b0f5df\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/_name_or_path                         : facebook/esm2_t6_8M_UR50D\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/add_cross_attention                   : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/architectures                         : ['EsmForMaskedLM']\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/attention_probs_dropout_prob          : 0.0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/attribute_map                         : {}\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/bad_words_ids                         : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/begin_suppress_tokens                 : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/bos_token_id                          : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/chunk_size_feed_forward               : 0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/classifier_dropout                    : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/cross_attention_hidden_size           : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/decoder_start_token_id                : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/diversity_penalty                     : 0.0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/do_sample                             : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/early_stopping                        : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/emb_layer_norm_before                 : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/encoder_no_repeat_ngram_size          : 0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/eos_token_id                          : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/esmfold_config                        : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/exponential_decay_length_penalty      : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/finetuning_task                       : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/forced_bos_token_id                   : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/forced_eos_token_id                   : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/hidden_act                            : gelu\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/hidden_dropout_prob                   : 0.0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/hidden_size                           : 320\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/id2label                              : {0: 'LABEL_0', 1: 'LABEL_1'}\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/initializer_range                     : 0.02\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/intermediate_size                     : 1280\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/is_composition                        : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/is_decoder                            : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/is_encoder_decoder                    : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/is_folding_model                      : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/label2id                              : {'LABEL_0': 0, 'LABEL_1': 1}\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/layer_norm_eps                        : 1e-05\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/length_penalty                        : 1.0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/mask_token_id                         : 32\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/max_length                            : 20\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/max_position_embeddings               : 1026\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/min_length                            : 0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/model_type                            : esm\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/name_or_path                          : facebook/esm2_t6_8M_UR50D\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/no_repeat_ngram_size                  : 0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/num_attention_heads                   : 20\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/num_beam_groups                       : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/num_beams                             : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/num_hidden_layers                     : 6\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/num_labels                            : 2\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/num_return_sequences                  : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/output_attentions                     : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/output_hidden_states                  : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/output_scores                         : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/pad_token_id                          : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/position_embedding_type               : rotary\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/prefix                                : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/problem_type                          : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/pruned_heads                          : {}\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/remove_invalid_values                 : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/repetition_penalty                    : 1.0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/return_dict                           : True\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/return_dict_in_generate               : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/sep_token_id                          : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/suppress_tokens                       : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/task_specific_params                  : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/temperature                           : 1.0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/tf_legacy_loss                        : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/tie_encoder_decoder                   : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/tie_word_embeddings                   : True\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/token_dropout                         : True\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/tokenizer_class                       : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/top_k                                 : 50\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/top_p                                 : 1.0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/torch_dtype                           : torch.float32\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/torchscript                           : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/transformers_version                  : 4.25.0.dev0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/typical_p                             : 1.0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/use_bfloat16                          : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/use_cache                             : True\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/use_return_dict                       : True\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/vocab_list                            : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/vocab_size                            : 33\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Uploads:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     conda-environment-definition : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     conda-info                   : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     conda-specification          : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     environment details          : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     filename                     : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     installed packages           : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model graph                  : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     notebook                     : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     os packages                  : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     source_code                  : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m \n",
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m To get all data logged automatically, import comet_ml before the following modules: torch, sklearn.\n",
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m As you are running in a Jupyter environment, you will need to call `experiment.end()` when finished to ensure all metrics and code are logged before exiting.\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Couldn't find a Git repository in '/home/mkoshkina/ESM' nor in any parent directory. Set `COMET_GIT_DIRECTORY` if your Git Repository is elsewhere.\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Experiment is live on comet.com https://www.comet.com/crystalcolecrystal/esm2-t6-8m-transfer-learning/214ed1343f2147e7ab4888844729581c\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7200' max='7200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7200/7200 2:16:01, Epoch 300/300]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.655652</td>\n",
       "      <td>0.643727</td>\n",
       "      <td>0.593750</td>\n",
       "      <td>0.690630</td>\n",
       "      <td>0.520706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.604424</td>\n",
       "      <td>0.705238</td>\n",
       "      <td>0.673194</td>\n",
       "      <td>0.755303</td>\n",
       "      <td>0.607186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.583107</td>\n",
       "      <td>0.710719</td>\n",
       "      <td>0.676651</td>\n",
       "      <td>0.766975</td>\n",
       "      <td>0.605359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.564244</td>\n",
       "      <td>0.722899</td>\n",
       "      <td>0.706072</td>\n",
       "      <td>0.751719</td>\n",
       "      <td>0.665652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.548066</td>\n",
       "      <td>0.729903</td>\n",
       "      <td>0.714699</td>\n",
       "      <td>0.757328</td>\n",
       "      <td>0.676614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.541569</td>\n",
       "      <td>0.732643</td>\n",
       "      <td>0.700546</td>\n",
       "      <td>0.796124</td>\n",
       "      <td>0.625457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.520412</td>\n",
       "      <td>0.745737</td>\n",
       "      <td>0.738490</td>\n",
       "      <td>0.760155</td>\n",
       "      <td>0.718027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.503139</td>\n",
       "      <td>0.752132</td>\n",
       "      <td>0.755849</td>\n",
       "      <td>0.744681</td>\n",
       "      <td>0.767357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.504033</td>\n",
       "      <td>0.757308</td>\n",
       "      <td>0.748183</td>\n",
       "      <td>0.777413</td>\n",
       "      <td>0.721072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.501440</td>\n",
       "      <td>0.760962</td>\n",
       "      <td>0.755528</td>\n",
       "      <td>0.773104</td>\n",
       "      <td>0.738733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.488906</td>\n",
       "      <td>0.768879</td>\n",
       "      <td>0.777093</td>\n",
       "      <td>0.750425</td>\n",
       "      <td>0.805725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.491838</td>\n",
       "      <td>0.769488</td>\n",
       "      <td>0.777156</td>\n",
       "      <td>0.752137</td>\n",
       "      <td>0.803898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.493412</td>\n",
       "      <td>0.771620</td>\n",
       "      <td>0.781086</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.814860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.499669</td>\n",
       "      <td>0.773752</td>\n",
       "      <td>0.778539</td>\n",
       "      <td>0.762405</td>\n",
       "      <td>0.795371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.511869</td>\n",
       "      <td>0.762789</td>\n",
       "      <td>0.780625</td>\n",
       "      <td>0.726035</td>\n",
       "      <td>0.844093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.516331</td>\n",
       "      <td>0.767966</td>\n",
       "      <td>0.770620</td>\n",
       "      <td>0.761905</td>\n",
       "      <td>0.779537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.522996</td>\n",
       "      <td>0.764007</td>\n",
       "      <td>0.774513</td>\n",
       "      <td>0.741504</td>\n",
       "      <td>0.810597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.534771</td>\n",
       "      <td>0.757613</td>\n",
       "      <td>0.773864</td>\n",
       "      <td>0.725240</td>\n",
       "      <td>0.829476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.544981</td>\n",
       "      <td>0.763398</td>\n",
       "      <td>0.766597</td>\n",
       "      <td>0.756372</td>\n",
       "      <td>0.777101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.557894</td>\n",
       "      <td>0.759135</td>\n",
       "      <td>0.767147</td>\n",
       "      <td>0.742450</td>\n",
       "      <td>0.793544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.459900</td>\n",
       "      <td>0.586319</td>\n",
       "      <td>0.754872</td>\n",
       "      <td>0.766328</td>\n",
       "      <td>0.732113</td>\n",
       "      <td>0.803898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.459900</td>\n",
       "      <td>0.596240</td>\n",
       "      <td>0.756395</td>\n",
       "      <td>0.761621</td>\n",
       "      <td>0.745624</td>\n",
       "      <td>0.778319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.459900</td>\n",
       "      <td>0.622507</td>\n",
       "      <td>0.747564</td>\n",
       "      <td>0.759082</td>\n",
       "      <td>0.725959</td>\n",
       "      <td>0.795371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.459900</td>\n",
       "      <td>0.638813</td>\n",
       "      <td>0.756090</td>\n",
       "      <td>0.760967</td>\n",
       "      <td>0.746050</td>\n",
       "      <td>0.776492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.459900</td>\n",
       "      <td>0.669522</td>\n",
       "      <td>0.745128</td>\n",
       "      <td>0.753896</td>\n",
       "      <td>0.728823</td>\n",
       "      <td>0.780755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.459900</td>\n",
       "      <td>0.690472</td>\n",
       "      <td>0.756090</td>\n",
       "      <td>0.766268</td>\n",
       "      <td>0.735574</td>\n",
       "      <td>0.799635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.459900</td>\n",
       "      <td>0.731639</td>\n",
       "      <td>0.742692</td>\n",
       "      <td>0.754859</td>\n",
       "      <td>0.720776</td>\n",
       "      <td>0.792326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.459900</td>\n",
       "      <td>0.745341</td>\n",
       "      <td>0.746041</td>\n",
       "      <td>0.755425</td>\n",
       "      <td>0.728507</td>\n",
       "      <td>0.784409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.459900</td>\n",
       "      <td>0.773301</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.759026</td>\n",
       "      <td>0.732578</td>\n",
       "      <td>0.787454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.459900</td>\n",
       "      <td>0.784880</td>\n",
       "      <td>0.750305</td>\n",
       "      <td>0.755807</td>\n",
       "      <td>0.739510</td>\n",
       "      <td>0.772838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>0.459900</td>\n",
       "      <td>0.828270</td>\n",
       "      <td>0.742387</td>\n",
       "      <td>0.743481</td>\n",
       "      <td>0.740338</td>\n",
       "      <td>0.746650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.459900</td>\n",
       "      <td>0.852602</td>\n",
       "      <td>0.738429</td>\n",
       "      <td>0.750363</td>\n",
       "      <td>0.717621</td>\n",
       "      <td>0.786236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>0.459900</td>\n",
       "      <td>0.878599</td>\n",
       "      <td>0.744823</td>\n",
       "      <td>0.746061</td>\n",
       "      <td>0.742461</td>\n",
       "      <td>0.749695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>0.459900</td>\n",
       "      <td>0.922008</td>\n",
       "      <td>0.738124</td>\n",
       "      <td>0.740495</td>\n",
       "      <td>0.733852</td>\n",
       "      <td>0.747259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.459900</td>\n",
       "      <td>0.932321</td>\n",
       "      <td>0.747259</td>\n",
       "      <td>0.753417</td>\n",
       "      <td>0.735499</td>\n",
       "      <td>0.772229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.459900</td>\n",
       "      <td>0.968826</td>\n",
       "      <td>0.737211</td>\n",
       "      <td>0.739982</td>\n",
       "      <td>0.732260</td>\n",
       "      <td>0.747868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>0.459900</td>\n",
       "      <td>0.991363</td>\n",
       "      <td>0.742996</td>\n",
       "      <td>0.745783</td>\n",
       "      <td>0.737783</td>\n",
       "      <td>0.753959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>0.459900</td>\n",
       "      <td>1.017892</td>\n",
       "      <td>0.731425</td>\n",
       "      <td>0.734017</td>\n",
       "      <td>0.727001</td>\n",
       "      <td>0.741169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>0.459900</td>\n",
       "      <td>1.041315</td>\n",
       "      <td>0.736602</td>\n",
       "      <td>0.743247</td>\n",
       "      <td>0.724957</td>\n",
       "      <td>0.762485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.459900</td>\n",
       "      <td>1.043719</td>\n",
       "      <td>0.740865</td>\n",
       "      <td>0.745742</td>\n",
       "      <td>0.731965</td>\n",
       "      <td>0.760049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>0.459900</td>\n",
       "      <td>1.085424</td>\n",
       "      <td>0.736906</td>\n",
       "      <td>0.738657</td>\n",
       "      <td>0.733774</td>\n",
       "      <td>0.743605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>0.139700</td>\n",
       "      <td>1.109119</td>\n",
       "      <td>0.735079</td>\n",
       "      <td>0.741686</td>\n",
       "      <td>0.723638</td>\n",
       "      <td>0.760658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>0.139700</td>\n",
       "      <td>1.123763</td>\n",
       "      <td>0.735384</td>\n",
       "      <td>0.732862</td>\n",
       "      <td>0.739913</td>\n",
       "      <td>0.725944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>0.139700</td>\n",
       "      <td>1.167528</td>\n",
       "      <td>0.724117</td>\n",
       "      <td>0.726449</td>\n",
       "      <td>0.720359</td>\n",
       "      <td>0.732643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.139700</td>\n",
       "      <td>1.147211</td>\n",
       "      <td>0.733557</td>\n",
       "      <td>0.738102</td>\n",
       "      <td>0.725721</td>\n",
       "      <td>0.750914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>0.139700</td>\n",
       "      <td>1.172751</td>\n",
       "      <td>0.725944</td>\n",
       "      <td>0.727768</td>\n",
       "      <td>0.722957</td>\n",
       "      <td>0.732643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>0.139700</td>\n",
       "      <td>1.197559</td>\n",
       "      <td>0.731121</td>\n",
       "      <td>0.732018</td>\n",
       "      <td>0.729583</td>\n",
       "      <td>0.734470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>0.139700</td>\n",
       "      <td>1.208063</td>\n",
       "      <td>0.735993</td>\n",
       "      <td>0.740030</td>\n",
       "      <td>0.728884</td>\n",
       "      <td>0.751523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>0.139700</td>\n",
       "      <td>1.215917</td>\n",
       "      <td>0.734775</td>\n",
       "      <td>0.736778</td>\n",
       "      <td>0.731254</td>\n",
       "      <td>0.742387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.139700</td>\n",
       "      <td>1.245157</td>\n",
       "      <td>0.732948</td>\n",
       "      <td>0.738755</td>\n",
       "      <td>0.723032</td>\n",
       "      <td>0.755177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>0.139700</td>\n",
       "      <td>1.211813</td>\n",
       "      <td>0.734775</td>\n",
       "      <td>0.736141</td>\n",
       "      <td>0.732369</td>\n",
       "      <td>0.739951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>0.139700</td>\n",
       "      <td>1.200662</td>\n",
       "      <td>0.742692</td>\n",
       "      <td>0.744017</td>\n",
       "      <td>0.740205</td>\n",
       "      <td>0.747868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>0.139700</td>\n",
       "      <td>1.267094</td>\n",
       "      <td>0.723812</td>\n",
       "      <td>0.727709</td>\n",
       "      <td>0.717584</td>\n",
       "      <td>0.738124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>0.139700</td>\n",
       "      <td>1.284597</td>\n",
       "      <td>0.722594</td>\n",
       "      <td>0.724023</td>\n",
       "      <td>0.720313</td>\n",
       "      <td>0.727771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>0.139700</td>\n",
       "      <td>1.274852</td>\n",
       "      <td>0.727771</td>\n",
       "      <td>0.732815</td>\n",
       "      <td>0.719484</td>\n",
       "      <td>0.746650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>0.139700</td>\n",
       "      <td>1.264476</td>\n",
       "      <td>0.728989</td>\n",
       "      <td>0.733053</td>\n",
       "      <td>0.722222</td>\n",
       "      <td>0.744214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>0.139700</td>\n",
       "      <td>1.261367</td>\n",
       "      <td>0.729903</td>\n",
       "      <td>0.727161</td>\n",
       "      <td>0.734618</td>\n",
       "      <td>0.719854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>0.139700</td>\n",
       "      <td>1.278064</td>\n",
       "      <td>0.731730</td>\n",
       "      <td>0.737876</td>\n",
       "      <td>0.721350</td>\n",
       "      <td>0.755177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>0.139700</td>\n",
       "      <td>1.303873</td>\n",
       "      <td>0.732034</td>\n",
       "      <td>0.730392</td>\n",
       "      <td>0.734895</td>\n",
       "      <td>0.725944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.139700</td>\n",
       "      <td>1.325570</td>\n",
       "      <td>0.721072</td>\n",
       "      <td>0.729474</td>\n",
       "      <td>0.708142</td>\n",
       "      <td>0.752132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>0.139700</td>\n",
       "      <td>1.290522</td>\n",
       "      <td>0.733861</td>\n",
       "      <td>0.734669</td>\n",
       "      <td>0.732446</td>\n",
       "      <td>0.736906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>0.139700</td>\n",
       "      <td>1.334915</td>\n",
       "      <td>0.721681</td>\n",
       "      <td>0.728461</td>\n",
       "      <td>0.711137</td>\n",
       "      <td>0.746650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>0.045900</td>\n",
       "      <td>1.294509</td>\n",
       "      <td>0.733252</td>\n",
       "      <td>0.734867</td>\n",
       "      <td>0.730445</td>\n",
       "      <td>0.739342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>0.045900</td>\n",
       "      <td>1.310879</td>\n",
       "      <td>0.729598</td>\n",
       "      <td>0.732530</td>\n",
       "      <td>0.724672</td>\n",
       "      <td>0.740560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>0.045900</td>\n",
       "      <td>1.300461</td>\n",
       "      <td>0.737211</td>\n",
       "      <td>0.743384</td>\n",
       "      <td>0.726322</td>\n",
       "      <td>0.761267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>0.045900</td>\n",
       "      <td>1.301111</td>\n",
       "      <td>0.731121</td>\n",
       "      <td>0.730546</td>\n",
       "      <td>0.732110</td>\n",
       "      <td>0.728989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67</td>\n",
       "      <td>0.045900</td>\n",
       "      <td>1.321898</td>\n",
       "      <td>0.728685</td>\n",
       "      <td>0.736935</td>\n",
       "      <td>0.715186</td>\n",
       "      <td>0.760049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>0.045900</td>\n",
       "      <td>1.323923</td>\n",
       "      <td>0.728685</td>\n",
       "      <td>0.732352</td>\n",
       "      <td>0.722584</td>\n",
       "      <td>0.742387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69</td>\n",
       "      <td>0.045900</td>\n",
       "      <td>1.309463</td>\n",
       "      <td>0.734470</td>\n",
       "      <td>0.737823</td>\n",
       "      <td>0.728622</td>\n",
       "      <td>0.747259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.045900</td>\n",
       "      <td>1.316692</td>\n",
       "      <td>0.731730</td>\n",
       "      <td>0.740654</td>\n",
       "      <td>0.716809</td>\n",
       "      <td>0.766139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71</td>\n",
       "      <td>0.045900</td>\n",
       "      <td>1.346617</td>\n",
       "      <td>0.728380</td>\n",
       "      <td>0.734682</td>\n",
       "      <td>0.718023</td>\n",
       "      <td>0.752132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>0.045900</td>\n",
       "      <td>1.338089</td>\n",
       "      <td>0.729903</td>\n",
       "      <td>0.736873</td>\n",
       "      <td>0.718334</td>\n",
       "      <td>0.756395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73</td>\n",
       "      <td>0.045900</td>\n",
       "      <td>1.340443</td>\n",
       "      <td>0.733252</td>\n",
       "      <td>0.738975</td>\n",
       "      <td>0.723454</td>\n",
       "      <td>0.755177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74</td>\n",
       "      <td>0.045900</td>\n",
       "      <td>1.343852</td>\n",
       "      <td>0.730816</td>\n",
       "      <td>0.729664</td>\n",
       "      <td>0.732801</td>\n",
       "      <td>0.726553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>0.045900</td>\n",
       "      <td>1.343999</td>\n",
       "      <td>0.723508</td>\n",
       "      <td>0.718012</td>\n",
       "      <td>0.732573</td>\n",
       "      <td>0.704019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>0.045900</td>\n",
       "      <td>1.338000</td>\n",
       "      <td>0.739038</td>\n",
       "      <td>0.747123</td>\n",
       "      <td>0.724671</td>\n",
       "      <td>0.771011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77</td>\n",
       "      <td>0.045900</td>\n",
       "      <td>1.337444</td>\n",
       "      <td>0.735079</td>\n",
       "      <td>0.747680</td>\n",
       "      <td>0.713732</td>\n",
       "      <td>0.785018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78</td>\n",
       "      <td>0.045900</td>\n",
       "      <td>1.323610</td>\n",
       "      <td>0.738733</td>\n",
       "      <td>0.741877</td>\n",
       "      <td>0.733056</td>\n",
       "      <td>0.750914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79</td>\n",
       "      <td>0.045900</td>\n",
       "      <td>1.366911</td>\n",
       "      <td>0.730816</td>\n",
       "      <td>0.734535</td>\n",
       "      <td>0.724526</td>\n",
       "      <td>0.744823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.045900</td>\n",
       "      <td>1.338281</td>\n",
       "      <td>0.734775</td>\n",
       "      <td>0.739611</td>\n",
       "      <td>0.726365</td>\n",
       "      <td>0.753350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81</td>\n",
       "      <td>0.045900</td>\n",
       "      <td>1.363668</td>\n",
       "      <td>0.724726</td>\n",
       "      <td>0.729503</td>\n",
       "      <td>0.717059</td>\n",
       "      <td>0.742387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82</td>\n",
       "      <td>0.045900</td>\n",
       "      <td>1.374456</td>\n",
       "      <td>0.729294</td>\n",
       "      <td>0.728550</td>\n",
       "      <td>0.730557</td>\n",
       "      <td>0.726553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83</td>\n",
       "      <td>0.045900</td>\n",
       "      <td>1.374957</td>\n",
       "      <td>0.727467</td>\n",
       "      <td>0.724531</td>\n",
       "      <td>0.732421</td>\n",
       "      <td>0.716809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84</td>\n",
       "      <td>0.035300</td>\n",
       "      <td>1.363139</td>\n",
       "      <td>0.739951</td>\n",
       "      <td>0.743235</td>\n",
       "      <td>0.733967</td>\n",
       "      <td>0.752741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>0.035300</td>\n",
       "      <td>1.354273</td>\n",
       "      <td>0.742996</td>\n",
       "      <td>0.747305</td>\n",
       "      <td>0.734982</td>\n",
       "      <td>0.760049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86</td>\n",
       "      <td>0.035300</td>\n",
       "      <td>1.394266</td>\n",
       "      <td>0.719245</td>\n",
       "      <td>0.720944</td>\n",
       "      <td>0.716606</td>\n",
       "      <td>0.725335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87</td>\n",
       "      <td>0.035300</td>\n",
       "      <td>1.385839</td>\n",
       "      <td>0.728076</td>\n",
       "      <td>0.731751</td>\n",
       "      <td>0.721992</td>\n",
       "      <td>0.741778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88</td>\n",
       "      <td>0.035300</td>\n",
       "      <td>1.348520</td>\n",
       "      <td>0.738733</td>\n",
       "      <td>0.742342</td>\n",
       "      <td>0.732227</td>\n",
       "      <td>0.752741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>89</td>\n",
       "      <td>0.035300</td>\n",
       "      <td>1.389023</td>\n",
       "      <td>0.723812</td>\n",
       "      <td>0.725401</td>\n",
       "      <td>0.721252</td>\n",
       "      <td>0.729598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.035300</td>\n",
       "      <td>1.386037</td>\n",
       "      <td>0.733252</td>\n",
       "      <td>0.732437</td>\n",
       "      <td>0.734681</td>\n",
       "      <td>0.730207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>91</td>\n",
       "      <td>0.035300</td>\n",
       "      <td>1.363085</td>\n",
       "      <td>0.735384</td>\n",
       "      <td>0.733517</td>\n",
       "      <td>0.738728</td>\n",
       "      <td>0.728380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92</td>\n",
       "      <td>0.035300</td>\n",
       "      <td>1.388045</td>\n",
       "      <td>0.732034</td>\n",
       "      <td>0.733010</td>\n",
       "      <td>0.730351</td>\n",
       "      <td>0.735688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93</td>\n",
       "      <td>0.035300</td>\n",
       "      <td>1.385058</td>\n",
       "      <td>0.730512</td>\n",
       "      <td>0.734951</td>\n",
       "      <td>0.723041</td>\n",
       "      <td>0.747259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94</td>\n",
       "      <td>0.035300</td>\n",
       "      <td>1.394029</td>\n",
       "      <td>0.726248</td>\n",
       "      <td>0.727162</td>\n",
       "      <td>0.724743</td>\n",
       "      <td>0.729598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>0.035300</td>\n",
       "      <td>1.423409</td>\n",
       "      <td>0.721985</td>\n",
       "      <td>0.732493</td>\n",
       "      <td>0.705816</td>\n",
       "      <td>0.761267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>0.035300</td>\n",
       "      <td>1.391807</td>\n",
       "      <td>0.726553</td>\n",
       "      <td>0.723352</td>\n",
       "      <td>0.731920</td>\n",
       "      <td>0.714982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97</td>\n",
       "      <td>0.035300</td>\n",
       "      <td>1.392558</td>\n",
       "      <td>0.725944</td>\n",
       "      <td>0.726277</td>\n",
       "      <td>0.725395</td>\n",
       "      <td>0.727162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98</td>\n",
       "      <td>0.035300</td>\n",
       "      <td>1.420145</td>\n",
       "      <td>0.731121</td>\n",
       "      <td>0.734596</td>\n",
       "      <td>0.725223</td>\n",
       "      <td>0.744214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99</td>\n",
       "      <td>0.035300</td>\n",
       "      <td>1.408169</td>\n",
       "      <td>0.728076</td>\n",
       "      <td>0.727162</td>\n",
       "      <td>0.729614</td>\n",
       "      <td>0.724726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.035300</td>\n",
       "      <td>1.379649</td>\n",
       "      <td>0.732948</td>\n",
       "      <td>0.732866</td>\n",
       "      <td>0.733090</td>\n",
       "      <td>0.732643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>101</td>\n",
       "      <td>0.035300</td>\n",
       "      <td>1.438504</td>\n",
       "      <td>0.722594</td>\n",
       "      <td>0.719260</td>\n",
       "      <td>0.728010</td>\n",
       "      <td>0.710719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>102</td>\n",
       "      <td>0.035300</td>\n",
       "      <td>1.417783</td>\n",
       "      <td>0.731425</td>\n",
       "      <td>0.742407</td>\n",
       "      <td>0.713244</td>\n",
       "      <td>0.774056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>103</td>\n",
       "      <td>0.035300</td>\n",
       "      <td>1.407522</td>\n",
       "      <td>0.725335</td>\n",
       "      <td>0.727492</td>\n",
       "      <td>0.721823</td>\n",
       "      <td>0.733252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>104</td>\n",
       "      <td>0.035300</td>\n",
       "      <td>1.421066</td>\n",
       "      <td>0.727467</td>\n",
       "      <td>0.734500</td>\n",
       "      <td>0.716021</td>\n",
       "      <td>0.753959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>105</td>\n",
       "      <td>0.030600</td>\n",
       "      <td>1.413335</td>\n",
       "      <td>0.739342</td>\n",
       "      <td>0.745692</td>\n",
       "      <td>0.727958</td>\n",
       "      <td>0.764312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>106</td>\n",
       "      <td>0.030600</td>\n",
       "      <td>1.448241</td>\n",
       "      <td>0.726857</td>\n",
       "      <td>0.731839</td>\n",
       "      <td>0.718732</td>\n",
       "      <td>0.745432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>107</td>\n",
       "      <td>0.030600</td>\n",
       "      <td>1.449196</td>\n",
       "      <td>0.726857</td>\n",
       "      <td>0.731839</td>\n",
       "      <td>0.718732</td>\n",
       "      <td>0.745432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>108</td>\n",
       "      <td>0.030600</td>\n",
       "      <td>1.427748</td>\n",
       "      <td>0.729598</td>\n",
       "      <td>0.735084</td>\n",
       "      <td>0.720468</td>\n",
       "      <td>0.750305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>109</td>\n",
       "      <td>0.030600</td>\n",
       "      <td>1.445274</td>\n",
       "      <td>0.725335</td>\n",
       "      <td>0.732344</td>\n",
       "      <td>0.714120</td>\n",
       "      <td>0.751523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.030600</td>\n",
       "      <td>1.444364</td>\n",
       "      <td>0.722290</td>\n",
       "      <td>0.729216</td>\n",
       "      <td>0.711472</td>\n",
       "      <td>0.747868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>111</td>\n",
       "      <td>0.030600</td>\n",
       "      <td>1.473433</td>\n",
       "      <td>0.729598</td>\n",
       "      <td>0.735399</td>\n",
       "      <td>0.719953</td>\n",
       "      <td>0.751523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>112</td>\n",
       "      <td>0.030600</td>\n",
       "      <td>1.441827</td>\n",
       "      <td>0.733252</td>\n",
       "      <td>0.738195</td>\n",
       "      <td>0.724765</td>\n",
       "      <td>0.752132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>113</td>\n",
       "      <td>0.030600</td>\n",
       "      <td>1.458692</td>\n",
       "      <td>0.728076</td>\n",
       "      <td>0.736500</td>\n",
       "      <td>0.714367</td>\n",
       "      <td>0.760049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>114</td>\n",
       "      <td>0.030600</td>\n",
       "      <td>1.440952</td>\n",
       "      <td>0.732948</td>\n",
       "      <td>0.733677</td>\n",
       "      <td>0.731678</td>\n",
       "      <td>0.735688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>115</td>\n",
       "      <td>0.030600</td>\n",
       "      <td>1.467141</td>\n",
       "      <td>0.729903</td>\n",
       "      <td>0.736247</td>\n",
       "      <td>0.719349</td>\n",
       "      <td>0.753959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>116</td>\n",
       "      <td>0.030600</td>\n",
       "      <td>1.436340</td>\n",
       "      <td>0.732948</td>\n",
       "      <td>0.736558</td>\n",
       "      <td>0.726734</td>\n",
       "      <td>0.746650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>117</td>\n",
       "      <td>0.030600</td>\n",
       "      <td>1.433503</td>\n",
       "      <td>0.733861</td>\n",
       "      <td>0.737222</td>\n",
       "      <td>0.728029</td>\n",
       "      <td>0.746650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>118</td>\n",
       "      <td>0.030600</td>\n",
       "      <td>1.451131</td>\n",
       "      <td>0.730207</td>\n",
       "      <td>0.735206</td>\n",
       "      <td>0.721831</td>\n",
       "      <td>0.749086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>119</td>\n",
       "      <td>0.030600</td>\n",
       "      <td>1.477697</td>\n",
       "      <td>0.730207</td>\n",
       "      <td>0.739258</td>\n",
       "      <td>0.715262</td>\n",
       "      <td>0.764921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.030600</td>\n",
       "      <td>1.463232</td>\n",
       "      <td>0.726248</td>\n",
       "      <td>0.731722</td>\n",
       "      <td>0.717379</td>\n",
       "      <td>0.746650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>121</td>\n",
       "      <td>0.030600</td>\n",
       "      <td>1.464776</td>\n",
       "      <td>0.725944</td>\n",
       "      <td>0.729405</td>\n",
       "      <td>0.720309</td>\n",
       "      <td>0.738733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>122</td>\n",
       "      <td>0.030600</td>\n",
       "      <td>1.494102</td>\n",
       "      <td>0.716809</td>\n",
       "      <td>0.722554</td>\n",
       "      <td>0.708187</td>\n",
       "      <td>0.737515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>123</td>\n",
       "      <td>0.030600</td>\n",
       "      <td>1.481849</td>\n",
       "      <td>0.729903</td>\n",
       "      <td>0.733393</td>\n",
       "      <td>0.724036</td>\n",
       "      <td>0.742996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>124</td>\n",
       "      <td>0.030600</td>\n",
       "      <td>1.476446</td>\n",
       "      <td>0.724117</td>\n",
       "      <td>0.725787</td>\n",
       "      <td>0.721420</td>\n",
       "      <td>0.730207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>0.027600</td>\n",
       "      <td>1.515887</td>\n",
       "      <td>0.726248</td>\n",
       "      <td>0.734730</td>\n",
       "      <td>0.712650</td>\n",
       "      <td>0.758222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>126</td>\n",
       "      <td>0.027600</td>\n",
       "      <td>1.447559</td>\n",
       "      <td>0.734775</td>\n",
       "      <td>0.737096</td>\n",
       "      <td>0.730700</td>\n",
       "      <td>0.743605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>127</td>\n",
       "      <td>0.027600</td>\n",
       "      <td>1.470218</td>\n",
       "      <td>0.733557</td>\n",
       "      <td>0.736842</td>\n",
       "      <td>0.727867</td>\n",
       "      <td>0.746041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>128</td>\n",
       "      <td>0.027600</td>\n",
       "      <td>1.481867</td>\n",
       "      <td>0.735384</td>\n",
       "      <td>0.741292</td>\n",
       "      <td>0.725102</td>\n",
       "      <td>0.758222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>129</td>\n",
       "      <td>0.027600</td>\n",
       "      <td>1.500966</td>\n",
       "      <td>0.728076</td>\n",
       "      <td>0.730456</td>\n",
       "      <td>0.724117</td>\n",
       "      <td>0.736906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.027600</td>\n",
       "      <td>1.482877</td>\n",
       "      <td>0.728380</td>\n",
       "      <td>0.733094</td>\n",
       "      <td>0.720588</td>\n",
       "      <td>0.746041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>131</td>\n",
       "      <td>0.027600</td>\n",
       "      <td>1.479696</td>\n",
       "      <td>0.728989</td>\n",
       "      <td>0.734328</td>\n",
       "      <td>0.720141</td>\n",
       "      <td>0.749086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>132</td>\n",
       "      <td>0.027600</td>\n",
       "      <td>1.488806</td>\n",
       "      <td>0.730512</td>\n",
       "      <td>0.734314</td>\n",
       "      <td>0.724097</td>\n",
       "      <td>0.744823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>133</td>\n",
       "      <td>0.027600</td>\n",
       "      <td>1.494724</td>\n",
       "      <td>0.726553</td>\n",
       "      <td>0.734947</td>\n",
       "      <td>0.713058</td>\n",
       "      <td>0.758222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>134</td>\n",
       "      <td>0.027600</td>\n",
       "      <td>1.502442</td>\n",
       "      <td>0.725639</td>\n",
       "      <td>0.730159</td>\n",
       "      <td>0.718326</td>\n",
       "      <td>0.742387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>135</td>\n",
       "      <td>0.027600</td>\n",
       "      <td>1.509186</td>\n",
       "      <td>0.728685</td>\n",
       "      <td>0.735058</td>\n",
       "      <td>0.718187</td>\n",
       "      <td>0.752741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>136</td>\n",
       "      <td>0.027600</td>\n",
       "      <td>1.494018</td>\n",
       "      <td>0.726553</td>\n",
       "      <td>0.733056</td>\n",
       "      <td>0.716028</td>\n",
       "      <td>0.750914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>137</td>\n",
       "      <td>0.027600</td>\n",
       "      <td>1.500720</td>\n",
       "      <td>0.736297</td>\n",
       "      <td>0.744241</td>\n",
       "      <td>0.722477</td>\n",
       "      <td>0.767357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>138</td>\n",
       "      <td>0.027600</td>\n",
       "      <td>1.497733</td>\n",
       "      <td>0.732034</td>\n",
       "      <td>0.738095</td>\n",
       "      <td>0.721769</td>\n",
       "      <td>0.755177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>139</td>\n",
       "      <td>0.027600</td>\n",
       "      <td>1.499931</td>\n",
       "      <td>0.731425</td>\n",
       "      <td>0.739053</td>\n",
       "      <td>0.718642</td>\n",
       "      <td>0.760658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.027600</td>\n",
       "      <td>1.490041</td>\n",
       "      <td>0.736906</td>\n",
       "      <td>0.740541</td>\n",
       "      <td>0.730450</td>\n",
       "      <td>0.750914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>141</td>\n",
       "      <td>0.027600</td>\n",
       "      <td>1.531157</td>\n",
       "      <td>0.731425</td>\n",
       "      <td>0.736874</td>\n",
       "      <td>0.722222</td>\n",
       "      <td>0.752132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>142</td>\n",
       "      <td>0.027600</td>\n",
       "      <td>1.529465</td>\n",
       "      <td>0.723203</td>\n",
       "      <td>0.728414</td>\n",
       "      <td>0.714956</td>\n",
       "      <td>0.742387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>143</td>\n",
       "      <td>0.027600</td>\n",
       "      <td>1.513695</td>\n",
       "      <td>0.728685</td>\n",
       "      <td>0.733313</td>\n",
       "      <td>0.721012</td>\n",
       "      <td>0.746041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>144</td>\n",
       "      <td>0.027600</td>\n",
       "      <td>1.526632</td>\n",
       "      <td>0.733861</td>\n",
       "      <td>0.742941</td>\n",
       "      <td>0.718430</td>\n",
       "      <td>0.769184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>145</td>\n",
       "      <td>0.027600</td>\n",
       "      <td>1.565840</td>\n",
       "      <td>0.726248</td>\n",
       "      <td>0.732679</td>\n",
       "      <td>0.715863</td>\n",
       "      <td>0.750305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>146</td>\n",
       "      <td>0.025400</td>\n",
       "      <td>1.522649</td>\n",
       "      <td>0.729903</td>\n",
       "      <td>0.732590</td>\n",
       "      <td>0.725373</td>\n",
       "      <td>0.739951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>147</td>\n",
       "      <td>0.025400</td>\n",
       "      <td>1.541480</td>\n",
       "      <td>0.730207</td>\n",
       "      <td>0.731515</td>\n",
       "      <td>0.727986</td>\n",
       "      <td>0.735079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>148</td>\n",
       "      <td>0.025400</td>\n",
       "      <td>1.520840</td>\n",
       "      <td>0.734166</td>\n",
       "      <td>0.741945</td>\n",
       "      <td>0.720850</td>\n",
       "      <td>0.764312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>149</td>\n",
       "      <td>0.025400</td>\n",
       "      <td>1.548767</td>\n",
       "      <td>0.724117</td>\n",
       "      <td>0.726614</td>\n",
       "      <td>0.720096</td>\n",
       "      <td>0.733252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.025400</td>\n",
       "      <td>1.527092</td>\n",
       "      <td>0.728685</td>\n",
       "      <td>0.732673</td>\n",
       "      <td>0.722058</td>\n",
       "      <td>0.743605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>151</td>\n",
       "      <td>0.025400</td>\n",
       "      <td>1.524687</td>\n",
       "      <td>0.729903</td>\n",
       "      <td>0.730968</td>\n",
       "      <td>0.728097</td>\n",
       "      <td>0.733861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>152</td>\n",
       "      <td>0.025400</td>\n",
       "      <td>1.531808</td>\n",
       "      <td>0.732948</td>\n",
       "      <td>0.739066</td>\n",
       "      <td>0.722513</td>\n",
       "      <td>0.756395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>153</td>\n",
       "      <td>0.025400</td>\n",
       "      <td>1.553941</td>\n",
       "      <td>0.728076</td>\n",
       "      <td>0.732394</td>\n",
       "      <td>0.720944</td>\n",
       "      <td>0.744214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>154</td>\n",
       "      <td>0.025400</td>\n",
       "      <td>1.565649</td>\n",
       "      <td>0.728380</td>\n",
       "      <td>0.730351</td>\n",
       "      <td>0.725090</td>\n",
       "      <td>0.735688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>155</td>\n",
       "      <td>0.025400</td>\n",
       "      <td>1.533722</td>\n",
       "      <td>0.740865</td>\n",
       "      <td>0.744828</td>\n",
       "      <td>0.733609</td>\n",
       "      <td>0.756395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>156</td>\n",
       "      <td>0.025400</td>\n",
       "      <td>1.550926</td>\n",
       "      <td>0.732948</td>\n",
       "      <td>0.736241</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.745432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>157</td>\n",
       "      <td>0.025400</td>\n",
       "      <td>1.544109</td>\n",
       "      <td>0.731730</td>\n",
       "      <td>0.738343</td>\n",
       "      <td>0.720580</td>\n",
       "      <td>0.757004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>158</td>\n",
       "      <td>0.025400</td>\n",
       "      <td>1.569136</td>\n",
       "      <td>0.730512</td>\n",
       "      <td>0.736999</td>\n",
       "      <td>0.719675</td>\n",
       "      <td>0.755177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>159</td>\n",
       "      <td>0.025400</td>\n",
       "      <td>1.553464</td>\n",
       "      <td>0.731730</td>\n",
       "      <td>0.738963</td>\n",
       "      <td>0.719561</td>\n",
       "      <td>0.759440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.025400</td>\n",
       "      <td>1.548314</td>\n",
       "      <td>0.737515</td>\n",
       "      <td>0.742225</td>\n",
       "      <td>0.729142</td>\n",
       "      <td>0.755786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>161</td>\n",
       "      <td>0.025400</td>\n",
       "      <td>1.556437</td>\n",
       "      <td>0.727162</td>\n",
       "      <td>0.725658</td>\n",
       "      <td>0.729680</td>\n",
       "      <td>0.721681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>162</td>\n",
       "      <td>0.025400</td>\n",
       "      <td>1.577317</td>\n",
       "      <td>0.724726</td>\n",
       "      <td>0.731751</td>\n",
       "      <td>0.713542</td>\n",
       "      <td>0.750914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>163</td>\n",
       "      <td>0.025400</td>\n",
       "      <td>1.570583</td>\n",
       "      <td>0.736906</td>\n",
       "      <td>0.745882</td>\n",
       "      <td>0.721274</td>\n",
       "      <td>0.772229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>164</td>\n",
       "      <td>0.025400</td>\n",
       "      <td>1.539690</td>\n",
       "      <td>0.730207</td>\n",
       "      <td>0.728886</td>\n",
       "      <td>0.732472</td>\n",
       "      <td>0.725335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>165</td>\n",
       "      <td>0.025400</td>\n",
       "      <td>1.550044</td>\n",
       "      <td>0.730512</td>\n",
       "      <td>0.737622</td>\n",
       "      <td>0.718660</td>\n",
       "      <td>0.757613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>166</td>\n",
       "      <td>0.025400</td>\n",
       "      <td>1.554769</td>\n",
       "      <td>0.728076</td>\n",
       "      <td>0.732073</td>\n",
       "      <td>0.721467</td>\n",
       "      <td>0.742996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>167</td>\n",
       "      <td>0.024000</td>\n",
       "      <td>1.583766</td>\n",
       "      <td>0.736906</td>\n",
       "      <td>0.744530</td>\n",
       "      <td>0.723563</td>\n",
       "      <td>0.766748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>168</td>\n",
       "      <td>0.024000</td>\n",
       "      <td>1.592051</td>\n",
       "      <td>0.730207</td>\n",
       "      <td>0.738489</td>\n",
       "      <td>0.716495</td>\n",
       "      <td>0.761876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>169</td>\n",
       "      <td>0.024000</td>\n",
       "      <td>1.585076</td>\n",
       "      <td>0.733861</td>\n",
       "      <td>0.740036</td>\n",
       "      <td>0.723256</td>\n",
       "      <td>0.757613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.024000</td>\n",
       "      <td>1.569555</td>\n",
       "      <td>0.731730</td>\n",
       "      <td>0.738343</td>\n",
       "      <td>0.720580</td>\n",
       "      <td>0.757004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>171</td>\n",
       "      <td>0.024000</td>\n",
       "      <td>1.554542</td>\n",
       "      <td>0.728685</td>\n",
       "      <td>0.733473</td>\n",
       "      <td>0.720752</td>\n",
       "      <td>0.746650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>172</td>\n",
       "      <td>0.024000</td>\n",
       "      <td>1.588215</td>\n",
       "      <td>0.725639</td>\n",
       "      <td>0.733668</td>\n",
       "      <td>0.712809</td>\n",
       "      <td>0.755786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>173</td>\n",
       "      <td>0.024000</td>\n",
       "      <td>1.572492</td>\n",
       "      <td>0.728685</td>\n",
       "      <td>0.729097</td>\n",
       "      <td>0.727990</td>\n",
       "      <td>0.730207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>174</td>\n",
       "      <td>0.024000</td>\n",
       "      <td>1.571960</td>\n",
       "      <td>0.733861</td>\n",
       "      <td>0.736905</td>\n",
       "      <td>0.728571</td>\n",
       "      <td>0.745432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175</td>\n",
       "      <td>0.024000</td>\n",
       "      <td>1.586153</td>\n",
       "      <td>0.728380</td>\n",
       "      <td>0.731971</td>\n",
       "      <td>0.722420</td>\n",
       "      <td>0.741778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>176</td>\n",
       "      <td>0.024000</td>\n",
       "      <td>1.554023</td>\n",
       "      <td>0.733861</td>\n",
       "      <td>0.736270</td>\n",
       "      <td>0.729665</td>\n",
       "      <td>0.742996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>177</td>\n",
       "      <td>0.024000</td>\n",
       "      <td>1.604506</td>\n",
       "      <td>0.732339</td>\n",
       "      <td>0.733232</td>\n",
       "      <td>0.730792</td>\n",
       "      <td>0.735688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>178</td>\n",
       "      <td>0.024000</td>\n",
       "      <td>1.599393</td>\n",
       "      <td>0.727467</td>\n",
       "      <td>0.732916</td>\n",
       "      <td>0.718549</td>\n",
       "      <td>0.747868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>179</td>\n",
       "      <td>0.024000</td>\n",
       "      <td>1.592414</td>\n",
       "      <td>0.729294</td>\n",
       "      <td>0.737681</td>\n",
       "      <td>0.715512</td>\n",
       "      <td>0.761267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.024000</td>\n",
       "      <td>1.581083</td>\n",
       "      <td>0.731730</td>\n",
       "      <td>0.739580</td>\n",
       "      <td>0.718553</td>\n",
       "      <td>0.761876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>181</td>\n",
       "      <td>0.024000</td>\n",
       "      <td>1.617190</td>\n",
       "      <td>0.724117</td>\n",
       "      <td>0.729067</td>\n",
       "      <td>0.716216</td>\n",
       "      <td>0.742387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>182</td>\n",
       "      <td>0.024000</td>\n",
       "      <td>1.598722</td>\n",
       "      <td>0.738429</td>\n",
       "      <td>0.746233</td>\n",
       "      <td>0.724613</td>\n",
       "      <td>0.769184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>183</td>\n",
       "      <td>0.024000</td>\n",
       "      <td>1.622385</td>\n",
       "      <td>0.724421</td>\n",
       "      <td>0.732328</td>\n",
       "      <td>0.711903</td>\n",
       "      <td>0.753959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>184</td>\n",
       "      <td>0.024000</td>\n",
       "      <td>1.584123</td>\n",
       "      <td>0.729903</td>\n",
       "      <td>0.733233</td>\n",
       "      <td>0.724302</td>\n",
       "      <td>0.742387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>185</td>\n",
       "      <td>0.024000</td>\n",
       "      <td>1.616088</td>\n",
       "      <td>0.727162</td>\n",
       "      <td>0.736160</td>\n",
       "      <td>0.712657</td>\n",
       "      <td>0.761267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>186</td>\n",
       "      <td>0.024000</td>\n",
       "      <td>1.604961</td>\n",
       "      <td>0.729598</td>\n",
       "      <td>0.735084</td>\n",
       "      <td>0.720468</td>\n",
       "      <td>0.750305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>187</td>\n",
       "      <td>0.024000</td>\n",
       "      <td>1.637617</td>\n",
       "      <td>0.724117</td>\n",
       "      <td>0.735243</td>\n",
       "      <td>0.706742</td>\n",
       "      <td>0.766139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>188</td>\n",
       "      <td>0.022800</td>\n",
       "      <td>1.642781</td>\n",
       "      <td>0.724726</td>\n",
       "      <td>0.730471</td>\n",
       "      <td>0.715537</td>\n",
       "      <td>0.746041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>189</td>\n",
       "      <td>0.022800</td>\n",
       "      <td>1.602764</td>\n",
       "      <td>0.727162</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.717113</td>\n",
       "      <td>0.750305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>0.022800</td>\n",
       "      <td>1.606203</td>\n",
       "      <td>0.735079</td>\n",
       "      <td>0.737794</td>\n",
       "      <td>0.730310</td>\n",
       "      <td>0.745432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>191</td>\n",
       "      <td>0.022800</td>\n",
       "      <td>1.638068</td>\n",
       "      <td>0.731730</td>\n",
       "      <td>0.739888</td>\n",
       "      <td>0.718052</td>\n",
       "      <td>0.763094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>192</td>\n",
       "      <td>0.022800</td>\n",
       "      <td>1.619361</td>\n",
       "      <td>0.732034</td>\n",
       "      <td>0.734620</td>\n",
       "      <td>0.727599</td>\n",
       "      <td>0.741778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>193</td>\n",
       "      <td>0.022800</td>\n",
       "      <td>1.623240</td>\n",
       "      <td>0.723203</td>\n",
       "      <td>0.731621</td>\n",
       "      <td>0.710029</td>\n",
       "      <td>0.754568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>194</td>\n",
       "      <td>0.022800</td>\n",
       "      <td>1.634486</td>\n",
       "      <td>0.726248</td>\n",
       "      <td>0.732838</td>\n",
       "      <td>0.715612</td>\n",
       "      <td>0.750914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>195</td>\n",
       "      <td>0.022800</td>\n",
       "      <td>1.610535</td>\n",
       "      <td>0.728380</td>\n",
       "      <td>0.732774</td>\n",
       "      <td>0.721108</td>\n",
       "      <td>0.744823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>196</td>\n",
       "      <td>0.022800</td>\n",
       "      <td>1.622625</td>\n",
       "      <td>0.734166</td>\n",
       "      <td>0.738701</td>\n",
       "      <td>0.726310</td>\n",
       "      <td>0.751523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>197</td>\n",
       "      <td>0.022800</td>\n",
       "      <td>1.657281</td>\n",
       "      <td>0.726553</td>\n",
       "      <td>0.732100</td>\n",
       "      <td>0.717544</td>\n",
       "      <td>0.747259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>198</td>\n",
       "      <td>0.022800</td>\n",
       "      <td>1.622900</td>\n",
       "      <td>0.727771</td>\n",
       "      <td>0.732656</td>\n",
       "      <td>0.719741</td>\n",
       "      <td>0.746041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>199</td>\n",
       "      <td>0.022800</td>\n",
       "      <td>1.618265</td>\n",
       "      <td>0.730816</td>\n",
       "      <td>0.737218</td>\n",
       "      <td>0.720093</td>\n",
       "      <td>0.755177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.022800</td>\n",
       "      <td>1.633756</td>\n",
       "      <td>0.720767</td>\n",
       "      <td>0.723545</td>\n",
       "      <td>0.716418</td>\n",
       "      <td>0.730816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>201</td>\n",
       "      <td>0.022800</td>\n",
       "      <td>1.629360</td>\n",
       "      <td>0.730207</td>\n",
       "      <td>0.737092</td>\n",
       "      <td>0.718750</td>\n",
       "      <td>0.756395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>202</td>\n",
       "      <td>0.022800</td>\n",
       "      <td>1.659377</td>\n",
       "      <td>0.723508</td>\n",
       "      <td>0.728631</td>\n",
       "      <td>0.715376</td>\n",
       "      <td>0.742387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>203</td>\n",
       "      <td>0.022800</td>\n",
       "      <td>1.642115</td>\n",
       "      <td>0.727162</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.717113</td>\n",
       "      <td>0.750305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>204</td>\n",
       "      <td>0.022800</td>\n",
       "      <td>1.624843</td>\n",
       "      <td>0.732643</td>\n",
       "      <td>0.733778</td>\n",
       "      <td>0.730676</td>\n",
       "      <td>0.736906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>205</td>\n",
       "      <td>0.022800</td>\n",
       "      <td>1.633885</td>\n",
       "      <td>0.726248</td>\n",
       "      <td>0.730273</td>\n",
       "      <td>0.719692</td>\n",
       "      <td>0.741169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>206</td>\n",
       "      <td>0.022800</td>\n",
       "      <td>1.650449</td>\n",
       "      <td>0.729903</td>\n",
       "      <td>0.734511</td>\n",
       "      <td>0.722190</td>\n",
       "      <td>0.747259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>207</td>\n",
       "      <td>0.022800</td>\n",
       "      <td>1.641525</td>\n",
       "      <td>0.728076</td>\n",
       "      <td>0.731429</td>\n",
       "      <td>0.722519</td>\n",
       "      <td>0.740560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>208</td>\n",
       "      <td>0.022800</td>\n",
       "      <td>1.632836</td>\n",
       "      <td>0.734166</td>\n",
       "      <td>0.738857</td>\n",
       "      <td>0.726044</td>\n",
       "      <td>0.752132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>209</td>\n",
       "      <td>0.021700</td>\n",
       "      <td>1.653117</td>\n",
       "      <td>0.732339</td>\n",
       "      <td>0.736274</td>\n",
       "      <td>0.725606</td>\n",
       "      <td>0.747259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>0.021700</td>\n",
       "      <td>1.654868</td>\n",
       "      <td>0.724117</td>\n",
       "      <td>0.733686</td>\n",
       "      <td>0.709091</td>\n",
       "      <td>0.760049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>211</td>\n",
       "      <td>0.021700</td>\n",
       "      <td>1.660393</td>\n",
       "      <td>0.729903</td>\n",
       "      <td>0.738271</td>\n",
       "      <td>0.716085</td>\n",
       "      <td>0.761876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>212</td>\n",
       "      <td>0.021700</td>\n",
       "      <td>1.661374</td>\n",
       "      <td>0.728685</td>\n",
       "      <td>0.732191</td>\n",
       "      <td>0.722849</td>\n",
       "      <td>0.741778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>213</td>\n",
       "      <td>0.021700</td>\n",
       "      <td>1.653316</td>\n",
       "      <td>0.725944</td>\n",
       "      <td>0.730054</td>\n",
       "      <td>0.719267</td>\n",
       "      <td>0.741169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>214</td>\n",
       "      <td>0.021700</td>\n",
       "      <td>1.638063</td>\n",
       "      <td>0.730207</td>\n",
       "      <td>0.732649</td>\n",
       "      <td>0.726077</td>\n",
       "      <td>0.739342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>215</td>\n",
       "      <td>0.021700</td>\n",
       "      <td>1.650592</td>\n",
       "      <td>0.730207</td>\n",
       "      <td>0.733293</td>\n",
       "      <td>0.725000</td>\n",
       "      <td>0.741778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>216</td>\n",
       "      <td>0.021700</td>\n",
       "      <td>1.670500</td>\n",
       "      <td>0.723508</td>\n",
       "      <td>0.725845</td>\n",
       "      <td>0.719760</td>\n",
       "      <td>0.732034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>217</td>\n",
       "      <td>0.021700</td>\n",
       "      <td>1.656595</td>\n",
       "      <td>0.725639</td>\n",
       "      <td>0.728696</td>\n",
       "      <td>0.720667</td>\n",
       "      <td>0.736906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>218</td>\n",
       "      <td>0.021700</td>\n",
       "      <td>1.668550</td>\n",
       "      <td>0.730512</td>\n",
       "      <td>0.734633</td>\n",
       "      <td>0.723568</td>\n",
       "      <td>0.746041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>219</td>\n",
       "      <td>0.021700</td>\n",
       "      <td>1.665233</td>\n",
       "      <td>0.728685</td>\n",
       "      <td>0.731384</td>\n",
       "      <td>0.724179</td>\n",
       "      <td>0.738733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.021700</td>\n",
       "      <td>1.661415</td>\n",
       "      <td>0.725944</td>\n",
       "      <td>0.728752</td>\n",
       "      <td>0.721360</td>\n",
       "      <td>0.736297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>221</td>\n",
       "      <td>0.021700</td>\n",
       "      <td>1.655355</td>\n",
       "      <td>0.731121</td>\n",
       "      <td>0.734755</td>\n",
       "      <td>0.724956</td>\n",
       "      <td>0.744823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>222</td>\n",
       "      <td>0.021700</td>\n",
       "      <td>1.667034</td>\n",
       "      <td>0.729598</td>\n",
       "      <td>0.733653</td>\n",
       "      <td>0.722813</td>\n",
       "      <td>0.744823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>223</td>\n",
       "      <td>0.021700</td>\n",
       "      <td>1.676493</td>\n",
       "      <td>0.733557</td>\n",
       "      <td>0.739661</td>\n",
       "      <td>0.723095</td>\n",
       "      <td>0.757004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>224</td>\n",
       "      <td>0.021700</td>\n",
       "      <td>1.706227</td>\n",
       "      <td>0.726553</td>\n",
       "      <td>0.737119</td>\n",
       "      <td>0.709696</td>\n",
       "      <td>0.766748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>225</td>\n",
       "      <td>0.021700</td>\n",
       "      <td>1.660499</td>\n",
       "      <td>0.730207</td>\n",
       "      <td>0.733774</td>\n",
       "      <td>0.724199</td>\n",
       "      <td>0.743605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>226</td>\n",
       "      <td>0.021700</td>\n",
       "      <td>1.682702</td>\n",
       "      <td>0.726248</td>\n",
       "      <td>0.728152</td>\n",
       "      <td>0.723123</td>\n",
       "      <td>0.733252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>227</td>\n",
       "      <td>0.021700</td>\n",
       "      <td>1.673384</td>\n",
       "      <td>0.728685</td>\n",
       "      <td>0.732834</td>\n",
       "      <td>0.721796</td>\n",
       "      <td>0.744214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>228</td>\n",
       "      <td>0.021700</td>\n",
       "      <td>1.677845</td>\n",
       "      <td>0.726248</td>\n",
       "      <td>0.733314</td>\n",
       "      <td>0.714864</td>\n",
       "      <td>0.752741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>229</td>\n",
       "      <td>0.021700</td>\n",
       "      <td>1.688997</td>\n",
       "      <td>0.727162</td>\n",
       "      <td>0.732057</td>\n",
       "      <td>0.719154</td>\n",
       "      <td>0.745432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>0.021000</td>\n",
       "      <td>1.679293</td>\n",
       "      <td>0.725639</td>\n",
       "      <td>0.730482</td>\n",
       "      <td>0.717813</td>\n",
       "      <td>0.743605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>231</td>\n",
       "      <td>0.021000</td>\n",
       "      <td>1.694706</td>\n",
       "      <td>0.731730</td>\n",
       "      <td>0.738032</td>\n",
       "      <td>0.721092</td>\n",
       "      <td>0.755786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>232</td>\n",
       "      <td>0.021000</td>\n",
       "      <td>1.681887</td>\n",
       "      <td>0.732643</td>\n",
       "      <td>0.738535</td>\n",
       "      <td>0.722611</td>\n",
       "      <td>0.755177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>233</td>\n",
       "      <td>0.021000</td>\n",
       "      <td>1.699294</td>\n",
       "      <td>0.729294</td>\n",
       "      <td>0.736123</td>\n",
       "      <td>0.718008</td>\n",
       "      <td>0.755177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>234</td>\n",
       "      <td>0.021000</td>\n",
       "      <td>1.689633</td>\n",
       "      <td>0.730512</td>\n",
       "      <td>0.733514</td>\n",
       "      <td>0.725432</td>\n",
       "      <td>0.741778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>235</td>\n",
       "      <td>0.021000</td>\n",
       "      <td>1.684305</td>\n",
       "      <td>0.729598</td>\n",
       "      <td>0.733653</td>\n",
       "      <td>0.722813</td>\n",
       "      <td>0.744823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>236</td>\n",
       "      <td>0.021000</td>\n",
       "      <td>1.708426</td>\n",
       "      <td>0.724421</td>\n",
       "      <td>0.729931</td>\n",
       "      <td>0.715623</td>\n",
       "      <td>0.744823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>237</td>\n",
       "      <td>0.021000</td>\n",
       "      <td>1.693361</td>\n",
       "      <td>0.734470</td>\n",
       "      <td>0.742164</td>\n",
       "      <td>0.721264</td>\n",
       "      <td>0.764312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>238</td>\n",
       "      <td>0.021000</td>\n",
       "      <td>1.700151</td>\n",
       "      <td>0.729598</td>\n",
       "      <td>0.736029</td>\n",
       "      <td>0.718931</td>\n",
       "      <td>0.753959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>239</td>\n",
       "      <td>0.021000</td>\n",
       "      <td>1.698993</td>\n",
       "      <td>0.728380</td>\n",
       "      <td>0.731163</td>\n",
       "      <td>0.723747</td>\n",
       "      <td>0.738733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.021000</td>\n",
       "      <td>1.700532</td>\n",
       "      <td>0.732034</td>\n",
       "      <td>0.739799</td>\n",
       "      <td>0.718966</td>\n",
       "      <td>0.761876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>241</td>\n",
       "      <td>0.021000</td>\n",
       "      <td>1.717104</td>\n",
       "      <td>0.730512</td>\n",
       "      <td>0.735742</td>\n",
       "      <td>0.721734</td>\n",
       "      <td>0.750305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>242</td>\n",
       "      <td>0.021000</td>\n",
       "      <td>1.704185</td>\n",
       "      <td>0.731425</td>\n",
       "      <td>0.739053</td>\n",
       "      <td>0.718642</td>\n",
       "      <td>0.760658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>243</td>\n",
       "      <td>0.021000</td>\n",
       "      <td>1.706368</td>\n",
       "      <td>0.731425</td>\n",
       "      <td>0.736716</td>\n",
       "      <td>0.722482</td>\n",
       "      <td>0.751523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>244</td>\n",
       "      <td>0.021000</td>\n",
       "      <td>1.712945</td>\n",
       "      <td>0.730816</td>\n",
       "      <td>0.736119</td>\n",
       "      <td>0.721897</td>\n",
       "      <td>0.750914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>245</td>\n",
       "      <td>0.021000</td>\n",
       "      <td>1.721156</td>\n",
       "      <td>0.729598</td>\n",
       "      <td>0.736029</td>\n",
       "      <td>0.718931</td>\n",
       "      <td>0.753959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>246</td>\n",
       "      <td>0.021000</td>\n",
       "      <td>1.713485</td>\n",
       "      <td>0.732034</td>\n",
       "      <td>0.736842</td>\n",
       "      <td>0.723854</td>\n",
       "      <td>0.750305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>247</td>\n",
       "      <td>0.021000</td>\n",
       "      <td>1.704735</td>\n",
       "      <td>0.730512</td>\n",
       "      <td>0.736529</td>\n",
       "      <td>0.720443</td>\n",
       "      <td>0.753350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>248</td>\n",
       "      <td>0.021000</td>\n",
       "      <td>1.721659</td>\n",
       "      <td>0.725944</td>\n",
       "      <td>0.731664</td>\n",
       "      <td>0.716706</td>\n",
       "      <td>0.747259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>249</td>\n",
       "      <td>0.021000</td>\n",
       "      <td>1.714958</td>\n",
       "      <td>0.728685</td>\n",
       "      <td>0.730897</td>\n",
       "      <td>0.724985</td>\n",
       "      <td>0.736906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.020200</td>\n",
       "      <td>1.726086</td>\n",
       "      <td>0.729903</td>\n",
       "      <td>0.735461</td>\n",
       "      <td>0.720631</td>\n",
       "      <td>0.750914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>251</td>\n",
       "      <td>0.020200</td>\n",
       "      <td>1.722054</td>\n",
       "      <td>0.727162</td>\n",
       "      <td>0.732856</td>\n",
       "      <td>0.717874</td>\n",
       "      <td>0.748477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>252</td>\n",
       "      <td>0.020200</td>\n",
       "      <td>1.726773</td>\n",
       "      <td>0.729598</td>\n",
       "      <td>0.735399</td>\n",
       "      <td>0.719953</td>\n",
       "      <td>0.751523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>253</td>\n",
       "      <td>0.020200</td>\n",
       "      <td>1.736421</td>\n",
       "      <td>0.731121</td>\n",
       "      <td>0.738370</td>\n",
       "      <td>0.718984</td>\n",
       "      <td>0.758831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>254</td>\n",
       "      <td>0.020200</td>\n",
       "      <td>1.722316</td>\n",
       "      <td>0.728685</td>\n",
       "      <td>0.732673</td>\n",
       "      <td>0.722058</td>\n",
       "      <td>0.743605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>255</td>\n",
       "      <td>0.020200</td>\n",
       "      <td>1.734667</td>\n",
       "      <td>0.728380</td>\n",
       "      <td>0.732934</td>\n",
       "      <td>0.720848</td>\n",
       "      <td>0.745432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>256</td>\n",
       "      <td>0.020200</td>\n",
       "      <td>1.731623</td>\n",
       "      <td>0.732948</td>\n",
       "      <td>0.739685</td>\n",
       "      <td>0.721482</td>\n",
       "      <td>0.758831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>257</td>\n",
       "      <td>0.020200</td>\n",
       "      <td>1.739892</td>\n",
       "      <td>0.730207</td>\n",
       "      <td>0.736623</td>\n",
       "      <td>0.719512</td>\n",
       "      <td>0.754568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>258</td>\n",
       "      <td>0.020200</td>\n",
       "      <td>1.728760</td>\n",
       "      <td>0.728076</td>\n",
       "      <td>0.731267</td>\n",
       "      <td>0.722784</td>\n",
       "      <td>0.739951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>259</td>\n",
       "      <td>0.020200</td>\n",
       "      <td>1.741138</td>\n",
       "      <td>0.726857</td>\n",
       "      <td>0.731678</td>\n",
       "      <td>0.718989</td>\n",
       "      <td>0.744823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>0.020200</td>\n",
       "      <td>1.737664</td>\n",
       "      <td>0.730207</td>\n",
       "      <td>0.736466</td>\n",
       "      <td>0.719767</td>\n",
       "      <td>0.753959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>261</td>\n",
       "      <td>0.020200</td>\n",
       "      <td>1.738762</td>\n",
       "      <td>0.729294</td>\n",
       "      <td>0.733433</td>\n",
       "      <td>0.722386</td>\n",
       "      <td>0.744823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>262</td>\n",
       "      <td>0.020200</td>\n",
       "      <td>1.745393</td>\n",
       "      <td>0.728076</td>\n",
       "      <td>0.732715</td>\n",
       "      <td>0.720424</td>\n",
       "      <td>0.745432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>263</td>\n",
       "      <td>0.020200</td>\n",
       "      <td>1.748772</td>\n",
       "      <td>0.732948</td>\n",
       "      <td>0.739840</td>\n",
       "      <td>0.721226</td>\n",
       "      <td>0.759440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>264</td>\n",
       "      <td>0.020200</td>\n",
       "      <td>1.751654</td>\n",
       "      <td>0.727467</td>\n",
       "      <td>0.732596</td>\n",
       "      <td>0.719062</td>\n",
       "      <td>0.746650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>265</td>\n",
       "      <td>0.020200</td>\n",
       "      <td>1.749434</td>\n",
       "      <td>0.732643</td>\n",
       "      <td>0.739466</td>\n",
       "      <td>0.721065</td>\n",
       "      <td>0.758831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>266</td>\n",
       "      <td>0.020200</td>\n",
       "      <td>1.750522</td>\n",
       "      <td>0.728380</td>\n",
       "      <td>0.733731</td>\n",
       "      <td>0.719555</td>\n",
       "      <td>0.748477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>267</td>\n",
       "      <td>0.020200</td>\n",
       "      <td>1.749950</td>\n",
       "      <td>0.728685</td>\n",
       "      <td>0.733791</td>\n",
       "      <td>0.720235</td>\n",
       "      <td>0.747868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>268</td>\n",
       "      <td>0.020200</td>\n",
       "      <td>1.757192</td>\n",
       "      <td>0.731121</td>\n",
       "      <td>0.737593</td>\n",
       "      <td>0.720255</td>\n",
       "      <td>0.755786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>269</td>\n",
       "      <td>0.020200</td>\n",
       "      <td>1.750627</td>\n",
       "      <td>0.728989</td>\n",
       "      <td>0.733692</td>\n",
       "      <td>0.721176</td>\n",
       "      <td>0.746650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>0.020200</td>\n",
       "      <td>1.755552</td>\n",
       "      <td>0.728076</td>\n",
       "      <td>0.732715</td>\n",
       "      <td>0.720424</td>\n",
       "      <td>0.745432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>271</td>\n",
       "      <td>0.019400</td>\n",
       "      <td>1.751875</td>\n",
       "      <td>0.729294</td>\n",
       "      <td>0.732471</td>\n",
       "      <td>0.723974</td>\n",
       "      <td>0.741169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>272</td>\n",
       "      <td>0.019400</td>\n",
       "      <td>1.761798</td>\n",
       "      <td>0.730207</td>\n",
       "      <td>0.736780</td>\n",
       "      <td>0.719258</td>\n",
       "      <td>0.755177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>273</td>\n",
       "      <td>0.019400</td>\n",
       "      <td>1.754572</td>\n",
       "      <td>0.727162</td>\n",
       "      <td>0.731254</td>\n",
       "      <td>0.720449</td>\n",
       "      <td>0.742387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>274</td>\n",
       "      <td>0.019400</td>\n",
       "      <td>1.761202</td>\n",
       "      <td>0.728989</td>\n",
       "      <td>0.734170</td>\n",
       "      <td>0.720399</td>\n",
       "      <td>0.748477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>275</td>\n",
       "      <td>0.019400</td>\n",
       "      <td>1.761184</td>\n",
       "      <td>0.727771</td>\n",
       "      <td>0.733611</td>\n",
       "      <td>0.718203</td>\n",
       "      <td>0.749695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>276</td>\n",
       "      <td>0.019400</td>\n",
       "      <td>1.764710</td>\n",
       "      <td>0.729294</td>\n",
       "      <td>0.734389</td>\n",
       "      <td>0.720821</td>\n",
       "      <td>0.748477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>277</td>\n",
       "      <td>0.019400</td>\n",
       "      <td>1.762844</td>\n",
       "      <td>0.731121</td>\n",
       "      <td>0.736654</td>\n",
       "      <td>0.721800</td>\n",
       "      <td>0.752132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>278</td>\n",
       "      <td>0.019400</td>\n",
       "      <td>1.768296</td>\n",
       "      <td>0.729294</td>\n",
       "      <td>0.734548</td>\n",
       "      <td>0.720562</td>\n",
       "      <td>0.749086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>279</td>\n",
       "      <td>0.019400</td>\n",
       "      <td>1.765576</td>\n",
       "      <td>0.728989</td>\n",
       "      <td>0.733852</td>\n",
       "      <td>0.720917</td>\n",
       "      <td>0.747259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.019400</td>\n",
       "      <td>1.767673</td>\n",
       "      <td>0.730512</td>\n",
       "      <td>0.736686</td>\n",
       "      <td>0.720186</td>\n",
       "      <td>0.753959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>281</td>\n",
       "      <td>0.019400</td>\n",
       "      <td>1.770213</td>\n",
       "      <td>0.727467</td>\n",
       "      <td>0.731634</td>\n",
       "      <td>0.720614</td>\n",
       "      <td>0.742996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>282</td>\n",
       "      <td>0.019400</td>\n",
       "      <td>1.772920</td>\n",
       "      <td>0.730207</td>\n",
       "      <td>0.735838</td>\n",
       "      <td>0.720794</td>\n",
       "      <td>0.751523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>283</td>\n",
       "      <td>0.019400</td>\n",
       "      <td>1.768194</td>\n",
       "      <td>0.729294</td>\n",
       "      <td>0.734389</td>\n",
       "      <td>0.720821</td>\n",
       "      <td>0.748477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>284</td>\n",
       "      <td>0.019400</td>\n",
       "      <td>1.777252</td>\n",
       "      <td>0.729903</td>\n",
       "      <td>0.735618</td>\n",
       "      <td>0.720374</td>\n",
       "      <td>0.751523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>285</td>\n",
       "      <td>0.019400</td>\n",
       "      <td>1.777018</td>\n",
       "      <td>0.728989</td>\n",
       "      <td>0.734645</td>\n",
       "      <td>0.719626</td>\n",
       "      <td>0.750305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>286</td>\n",
       "      <td>0.019400</td>\n",
       "      <td>1.775669</td>\n",
       "      <td>0.728685</td>\n",
       "      <td>0.733632</td>\n",
       "      <td>0.720493</td>\n",
       "      <td>0.747259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>287</td>\n",
       "      <td>0.019400</td>\n",
       "      <td>1.777774</td>\n",
       "      <td>0.730512</td>\n",
       "      <td>0.736686</td>\n",
       "      <td>0.720186</td>\n",
       "      <td>0.753959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>288</td>\n",
       "      <td>0.019400</td>\n",
       "      <td>1.779410</td>\n",
       "      <td>0.730816</td>\n",
       "      <td>0.736277</td>\n",
       "      <td>0.721637</td>\n",
       "      <td>0.751523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>289</td>\n",
       "      <td>0.019400</td>\n",
       "      <td>1.778547</td>\n",
       "      <td>0.729598</td>\n",
       "      <td>0.734925</td>\n",
       "      <td>0.720726</td>\n",
       "      <td>0.749695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>0.019400</td>\n",
       "      <td>1.779745</td>\n",
       "      <td>0.729294</td>\n",
       "      <td>0.734548</td>\n",
       "      <td>0.720562</td>\n",
       "      <td>0.749086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>291</td>\n",
       "      <td>0.019400</td>\n",
       "      <td>1.782557</td>\n",
       "      <td>0.729903</td>\n",
       "      <td>0.735303</td>\n",
       "      <td>0.720889</td>\n",
       "      <td>0.750305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>292</td>\n",
       "      <td>0.018700</td>\n",
       "      <td>1.781846</td>\n",
       "      <td>0.729598</td>\n",
       "      <td>0.734925</td>\n",
       "      <td>0.720726</td>\n",
       "      <td>0.749695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>293</td>\n",
       "      <td>0.018700</td>\n",
       "      <td>1.783608</td>\n",
       "      <td>0.728989</td>\n",
       "      <td>0.734011</td>\n",
       "      <td>0.720657</td>\n",
       "      <td>0.747868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>294</td>\n",
       "      <td>0.018700</td>\n",
       "      <td>1.783044</td>\n",
       "      <td>0.729294</td>\n",
       "      <td>0.734230</td>\n",
       "      <td>0.721080</td>\n",
       "      <td>0.747868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>295</td>\n",
       "      <td>0.018700</td>\n",
       "      <td>1.783074</td>\n",
       "      <td>0.728989</td>\n",
       "      <td>0.734011</td>\n",
       "      <td>0.720657</td>\n",
       "      <td>0.747868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>296</td>\n",
       "      <td>0.018700</td>\n",
       "      <td>1.783977</td>\n",
       "      <td>0.728380</td>\n",
       "      <td>0.733254</td>\n",
       "      <td>0.720329</td>\n",
       "      <td>0.746650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>297</td>\n",
       "      <td>0.018700</td>\n",
       "      <td>1.784417</td>\n",
       "      <td>0.729294</td>\n",
       "      <td>0.734389</td>\n",
       "      <td>0.720821</td>\n",
       "      <td>0.748477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>298</td>\n",
       "      <td>0.018700</td>\n",
       "      <td>1.784506</td>\n",
       "      <td>0.729294</td>\n",
       "      <td>0.734389</td>\n",
       "      <td>0.720821</td>\n",
       "      <td>0.748477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>299</td>\n",
       "      <td>0.018700</td>\n",
       "      <td>1.784284</td>\n",
       "      <td>0.728685</td>\n",
       "      <td>0.733632</td>\n",
       "      <td>0.720493</td>\n",
       "      <td>0.747259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.018700</td>\n",
       "      <td>1.784383</td>\n",
       "      <td>0.728989</td>\n",
       "      <td>0.734011</td>\n",
       "      <td>0.720657</td>\n",
       "      <td>0.747868</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m ---------------------------------------------------------------------------------------\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Comet.ml Experiment Summary\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m ---------------------------------------------------------------------------------------\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Data:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     display_summary_level : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     name                  : batch_size_test\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     url                   : https://www.comet.com/crystalcolecrystal/esm2-t6-8m-transfer-learning/214ed1343f2147e7ab4888844729581c\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Metrics [count] (min, max):\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     epoch [315]                   : (1.0, 300.0)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     eval_accuracy [300]           : (0.643727161997564, 0.7737515225334958)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     eval_f1 [300]                 : (0.5937499999999999, 0.7810858143607706)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     eval_loss [300]               : (0.48890572786331177, 1.7845057249069214)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     eval_precision [300]          : (0.6906300484652665, 0.796124031007752)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     eval_recall [300]             : (0.5207064555420219, 0.8440925700365408)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     eval_runtime [300]            : (0.664, 7.1186)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     eval_samples_per_second [300] : (461.329, 4945.944)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     eval_steps_per_second [300]   : (0.281, 3.012)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     grad_norm [14]                : (0.9687374234199524, 4.3814697265625)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     learning_rate [14]            : (5.555555555555555e-07, 1.8611111111111114e-05)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     loss [14]                     : (0.0187, 0.4599)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     total_flos                    : 3.3920592132368136e+16\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     train_loss                    : 0.06385186142391629\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     train_runtime                 : 6551.6953\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     train_samples_per_second      : 2244.335\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     train_steps_per_second        : 1.099\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Others:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     Name : batch_size_test\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Parameters:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/_n_gpu                                  : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/_no_sync_in_gradient_accumulation       : True\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/_setup_devices                          : cuda:0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/accelerator_config                      : AcceleratorConfig(split_batches=False, dispatch_batches=None, even_batches=True, use_seedable_sampler=True, non_blocking=False, gradient_accumulation_kwargs=None)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/adafactor                               : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/adam_beta1                              : 0.9\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/adam_beta2                              : 0.999\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/adam_epsilon                            : 1e-08\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/auto_find_batch_size                    : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/batch_eval_metrics                      : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/bf16                                    : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/bf16_full_eval                          : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/data_seed                               : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/dataloader_drop_last                    : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/dataloader_num_workers                  : 0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/dataloader_persistent_workers           : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/dataloader_pin_memory                   : True\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/dataloader_prefetch_factor              : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/ddp_backend                             : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/ddp_broadcast_buffers                   : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/ddp_bucket_cap_mb                       : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/ddp_find_unused_parameters              : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/ddp_timeout                             : 1800\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/ddp_timeout_delta                       : 0:30:00\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/debug                                   : []\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/deepspeed                               : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/deepspeed_plugin                        : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/default_optim                           : adamw_torch\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/device                                  : cuda:0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/disable_tqdm                            : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/dispatch_batches                        : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/distributed_state                       : Distributed environment: DistributedType.NO\n",
      "Num processes: 1\n",
      "Process index: 0\n",
      "Local process index: 0\n",
      "Device: cuda\n",
      "\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/do_eval                                 : True\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/do_predict                              : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/do_train                                : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/eval_accumulation_steps                 : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/eval_batch_size                         : 2048\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/eval_delay                              : 0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/eval_do_concat_batches                  : True\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/eval_steps                              : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/eval_strategy                           : IntervalStrategy.EPOCH\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/evaluation_strategy                     : epoch\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/fp16                                    : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/fp16_backend                            : auto\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/fp16_full_eval                          : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/fp16_opt_level                          : O1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/framework                               : pt\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/fsdp                                    : []\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/fsdp_config                             : {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/fsdp_min_num_params                     : 0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/fsdp_transformer_layer_cls_to_wrap      : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/full_determinism                        : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/gradient_accumulation_steps             : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/gradient_checkpointing                  : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/gradient_checkpointing_kwargs           : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/greater_is_better                       : True\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/group_by_length                         : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/half_precision_backend                  : auto\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/hub_always_push                         : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/hub_model_id                            : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/hub_private_repo                        : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/hub_strategy                            : HubStrategy.EVERY_SAVE\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/hub_token                               : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/ignore_data_skip                        : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/include_inputs_for_metrics              : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/include_num_input_tokens_seen           : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/include_tokens_per_second               : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/jit_mode_eval                           : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/label_names                             : ['labels']\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/label_smoothing_factor                  : 0.0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/learning_rate                           : 2e-05\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/length_column_name                      : length\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/load_best_model_at_end                  : True\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/local_process_index                     : 0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/local_rank                              : 0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/log_level                               : passive\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/log_level_replica                       : warning\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/log_on_each_node                        : True\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/logging_dir                             : esm2_t6_8M_UR50D-batch-size-test-finetuned-localization/runs/Jun06_17-53-17_gpu-2-2-19\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/logging_first_step                      : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/logging_nan_inf_filter                  : True\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/logging_steps                           : 500\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/logging_strategy                        : IntervalStrategy.STEPS\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/lr_scheduler_kwargs                     : {}\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/lr_scheduler_type                       : SchedulerType.LINEAR\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/max_grad_norm                           : 1.0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/max_steps                               : -1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/metric_for_best_model                   : accuracy\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/mp_parameters                           : \n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/n_gpu                                   : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/neftune_noise_alpha                     : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/no_cuda                                 : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/num_train_epochs                        : 300\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/optim                                   : OptimizerNames.ADAMW_TORCH\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/optim_args                              : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/optim_target_modules                    : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/output_dir                              : esm2_t6_8M_UR50D-batch-size-test-finetuned-localization\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/overwrite_output_dir                    : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/parallel_mode                           : ParallelMode.NOT_PARALLEL\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/past_index                              : -1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/per_device_eval_batch_size              : 2048\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/per_device_train_batch_size             : 2048\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/per_gpu_eval_batch_size                 : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/per_gpu_train_batch_size                : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/place_model_on_device                   : True\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/prediction_loss_only                    : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/process_index                           : 0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/push_to_hub                             : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/push_to_hub_model_id                    : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/push_to_hub_organization                : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/push_to_hub_token                       : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/ray_scope                               : last\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/remove_unused_columns                   : True\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/report_to                               : ['comet_ml']\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/restore_callback_states_from_checkpoint : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/resume_from_checkpoint                  : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/run_name                                : esm2_t6_8M_UR50D-batch-size-test-finetuned-localization\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/save_on_each_node                       : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/save_only_model                         : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/save_safetensors                        : True\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/save_steps                              : 500\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/save_strategy                           : IntervalStrategy.EPOCH\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/save_total_limit                        : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/seed                                    : 42\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/should_log                              : True\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/should_save                             : True\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/skip_memory_metrics                     : True\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/split_batches                           : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/tf32                                    : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/torch_compile                           : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/torch_compile_backend                   : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/torch_compile_mode                      : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/torchdynamo                             : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/tpu_metrics_debug                       : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/tpu_num_cores                           : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/train_batch_size                        : 2048\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/use_cpu                                 : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/use_ipex                                : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/use_legacy_prediction_loop              : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/use_mps_device                          : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/warmup_ratio                            : 0.0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/warmup_steps                            : 0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/weight_decay                            : 0.01\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/world_size                              : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/_attn_implementation                  : eager\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/_attn_implementation_internal         : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/_auto_class                           : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/_commit_hash                          : c731040fcd8d73dceaa04b0a8e6329b345b0f5df\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/_name_or_path                         : facebook/esm2_t6_8M_UR50D\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/add_cross_attention                   : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/architectures                         : ['EsmForMaskedLM']\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/attention_probs_dropout_prob          : 0.0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/attribute_map                         : {}\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/bad_words_ids                         : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/begin_suppress_tokens                 : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/bos_token_id                          : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/chunk_size_feed_forward               : 0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/classifier_dropout                    : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/cross_attention_hidden_size           : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/decoder_start_token_id                : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/diversity_penalty                     : 0.0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/do_sample                             : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/early_stopping                        : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/emb_layer_norm_before                 : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/encoder_no_repeat_ngram_size          : 0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/eos_token_id                          : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/esmfold_config                        : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/exponential_decay_length_penalty      : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/finetuning_task                       : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/forced_bos_token_id                   : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/forced_eos_token_id                   : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/hidden_act                            : gelu\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/hidden_dropout_prob                   : 0.0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/hidden_size                           : 320\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/id2label                              : {0: 'LABEL_0', 1: 'LABEL_1'}\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/initializer_range                     : 0.02\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/intermediate_size                     : 1280\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/is_composition                        : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/is_decoder                            : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/is_encoder_decoder                    : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/is_folding_model                      : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/label2id                              : {'LABEL_0': 0, 'LABEL_1': 1}\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/layer_norm_eps                        : 1e-05\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/length_penalty                        : 1.0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/mask_token_id                         : 32\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/max_length                            : 20\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/max_position_embeddings               : 1026\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/min_length                            : 0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/model_type                            : esm\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/name_or_path                          : facebook/esm2_t6_8M_UR50D\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/no_repeat_ngram_size                  : 0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/num_attention_heads                   : 20\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/num_beam_groups                       : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/num_beams                             : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/num_hidden_layers                     : 6\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/num_labels                            : 2\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/num_return_sequences                  : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/output_attentions                     : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/output_hidden_states                  : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/output_scores                         : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/pad_token_id                          : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/position_embedding_type               : rotary\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/prefix                                : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/problem_type                          : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/pruned_heads                          : {}\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/remove_invalid_values                 : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/repetition_penalty                    : 1.0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/return_dict                           : True\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/return_dict_in_generate               : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/sep_token_id                          : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/suppress_tokens                       : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/task_specific_params                  : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/temperature                           : 1.0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/tf_legacy_loss                        : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/tie_encoder_decoder                   : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/tie_word_embeddings                   : True\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/token_dropout                         : True\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/tokenizer_class                       : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/top_k                                 : 50\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/top_p                                 : 1.0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/torch_dtype                           : torch.float32\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/torchscript                           : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/transformers_version                  : 4.25.0.dev0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/typical_p                             : 1.0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/use_bfloat16                          : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/use_cache                             : True\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/use_return_dict                       : True\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/vocab_list                            : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/vocab_size                            : 33\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Uploads:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     asset                        : 3000 (25.60 GB)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     conda-environment-definition : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     conda-info                   : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     conda-specification          : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     environment details          : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     filename                     : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     installed packages           : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model graph                  : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     notebook                     : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     os packages                  : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     source_code                  : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m \n",
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m To get all data logged automatically, import comet_ml before the following modules: torch, sklearn.\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Please wait for assets to finish uploading (timeout is 10800 seconds)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Still uploading 2747 file(s), remaining 24.33 GB/25.60 GB\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Still uploading 2747 asset(s), remaining 24.10 GB/25.60 GB, Throughput 15.81 MB/s, ETA ~1561s\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Still uploading 2690 asset(s), remaining 23.88 GB/25.16 GB, Throughput 14.88 MB/s, ETA ~1644s\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Still uploading 2607 asset(s), remaining 23.68 GB/24.78 GB, Throughput 13.72 MB/s, ETA ~1768s\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Still uploading 2593 asset(s), remaining 23.44 GB/24.69 GB, Throughput 15.88 MB/s, ETA ~1512s\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Still uploading 2587 asset(s), remaining 23.17 GB/24.67 GB, Throughput 18.93 MB/s, ETA ~1254s\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Still uploading 2587 asset(s), remaining 22.97 GB/24.67 GB, Throughput 13.73 MB/s, ETA ~1713s\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Still uploading 2587 asset(s), remaining 22.72 GB/24.67 GB, Throughput 16.64 MB/s, ETA ~1399s\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Still uploading 2583 asset(s), remaining 22.48 GB/24.61 GB, Throughput 16.42 MB/s, ETA ~1402s\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Still uploading 2553 asset(s), remaining 22.21 GB/24.24 GB, Throughput 18.45 MB/s, ETA ~1233s\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Still uploading 2429 asset(s), remaining 21.98 GB/23.14 GB, Throughput 15.99 MB/s, ETA ~1408s\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Still uploading 2377 asset(s), remaining 21.75 GB/22.66 GB, Throughput 15.50 MB/s, ETA ~1437s\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Still uploading 2353 asset(s), remaining 21.51 GB/22.46 GB, Throughput 16.54 MB/s, ETA ~1332s\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Still uploading 2352 asset(s), remaining 21.28 GB/22.40 GB, Throughput 15.50 MB/s, ETA ~1406s\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Still uploading 2347 asset(s), remaining 21.05 GB/22.40 GB, Throughput 15.68 MB/s, ETA ~1375s\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Still uploading 2347 asset(s), remaining 20.79 GB/22.40 GB, Throughput 17.44 MB/s, ETA ~1221s\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Still uploading 2335 asset(s), remaining 20.56 GB/22.29 GB, Throughput 15.58 MB/s, ETA ~1352s\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Still uploading 2282 asset(s), remaining 20.32 GB/21.86 GB, Throughput 16.77 MB/s, ETA ~1241s\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Still uploading 2206 asset(s), remaining 20.07 GB/21.26 GB, Throughput 16.57 MB/s, ETA ~1241s\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Still uploading 2157 asset(s), remaining 19.84 GB/20.89 GB, Throughput 15.77 MB/s, ETA ~1289s\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Still uploading 2153 asset(s), remaining 19.61 GB/20.83 GB, Throughput 15.92 MB/s, ETA ~1262s\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Still uploading 2149 asset(s), remaining 19.39 GB/20.80 GB, Throughput 15.15 MB/s, ETA ~1311s\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Still uploading 2147 asset(s), remaining 19.16 GB/20.80 GB, Throughput 15.37 MB/s, ETA ~1277s\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Still uploading 2147 asset(s), remaining 18.91 GB/20.80 GB, Throughput 17.22 MB/s, ETA ~1125s\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Still uploading 2112 asset(s), remaining 18.66 GB/20.43 GB, Throughput 17.02 MB/s, ETA ~1123s\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Still uploading 2060 asset(s), remaining 18.40 GB/20.01 GB, Throughput 17.73 MB/s, ETA ~1063s\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Still uploading 1969 asset(s), remaining 18.18 GB/19.21 GB, Throughput 14.68 MB/s, ETA ~1269s\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Still uploading 1938 asset(s), remaining 17.93 GB/18.96 GB, Throughput 16.99 MB/s, ETA ~1081s\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Still uploading 1933 asset(s), remaining 17.70 GB/18.90 GB, Throughput 15.84 MB/s, ETA ~1145s\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Still uploading 1927 asset(s), remaining 17.44 GB/18.87 GB, Throughput 17.80 MB/s, ETA ~1003s\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Still uploading 1917 asset(s), remaining 17.19 GB/18.76 GB, Throughput 17.00 MB/s, ETA ~1036s\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Still uploading 1905 asset(s), remaining 16.94 GB/18.62 GB, Throughput 16.57 MB/s, ETA ~1048s\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Still uploading 1861 asset(s), remaining 16.71 GB/18.25 GB, Throughput 15.74 MB/s, ETA ~1088s\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Still uploading 1792 asset(s), remaining 16.46 GB/17.65 GB, Throughput 16.68 MB/s, ETA ~1011s\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Still uploading 1747 asset(s), remaining 16.24 GB/17.36 GB, Throughput 15.42 MB/s, ETA ~1079s\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Still uploading 1723 asset(s), remaining 16.00 GB/17.14 GB, Throughput 15.98 MB/s, ETA ~1026s\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Still uploading 1723 asset(s), remaining 15.77 GB/17.14 GB, Throughput 15.61 MB/s, ETA ~1035s\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Still uploading 1717 asset(s), remaining 15.52 GB/17.11 GB, Throughput 17.39 MB/s, ETA ~914s\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Still uploading 1713 asset(s), remaining 15.28 GB/17.08 GB, Throughput 16.18 MB/s, ETA ~967s\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Still uploading 1680 asset(s), remaining 15.02 GB/16.79 GB, Throughput 17.49 MB/s, ETA ~880s\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Still uploading 1623 asset(s), remaining 14.79 GB/16.25 GB, Throughput 15.64 MB/s, ETA ~969s\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Still uploading 1561 asset(s), remaining 14.53 GB/15.68 GB, Throughput 17.89 MB/s, ETA ~832s\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Still uploading 1521 asset(s), remaining 14.29 GB/15.32 GB, Throughput 16.35 MB/s, ETA ~896s\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Still uploading 1513 asset(s), remaining 14.05 GB/15.32 GB, Throughput 15.87 MB/s, ETA ~907s\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Still uploading 1504 asset(s), remaining 13.82 GB/15.20 GB, Throughput 15.87 MB/s, ETA ~892s\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Still uploading 1495 asset(s), remaining 13.55 GB/15.17 GB, Throughput 18.55 MB/s, ETA ~748s\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Still uploading 1463 asset(s), remaining 13.31 GB/14.86 GB, Throughput 16.06 MB/s, ETA ~849s\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Still uploading 1427 asset(s), remaining 13.08 GB/14.58 GB, Throughput 15.72 MB/s, ETA ~853s\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Still uploading 1377 asset(s), remaining 12.81 GB/14.07 GB, Throughput 17.90 MB/s, ETA ~733s\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Still uploading 1325 asset(s), remaining 12.61 GB/13.72 GB, Throughput 14.18 MB/s, ETA ~911s\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Still uploading 1297 asset(s), remaining 12.33 GB/13.52 GB, Throughput 18.50 MB/s, ETA ~683s\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Still uploading 1293 asset(s), remaining 12.09 GB/13.47 GB, Throughput 16.75 MB/s, ETA ~739s\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Still uploading 1287 asset(s), remaining 11.84 GB/13.44 GB, Throughput 16.85 MB/s, ETA ~720s\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Still uploading 1265 asset(s), remaining 11.62 GB/13.18 GB, Throughput 14.81 MB/s, ETA ~804s\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Still uploading 1237 asset(s), remaining 11.36 GB/12.95 GB, Throughput 17.34 MB/s, ETA ~672s\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Still uploading 1193 asset(s), remaining 11.12 GB/12.64 GB, Throughput 16.83 MB/s, ETA ~677s\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Still uploading 1135 asset(s), remaining 10.87 GB/12.10 GB, Throughput 16.60 MB/s, ETA ~671s\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Still uploading 1105 asset(s), remaining 10.62 GB/11.87 GB, Throughput 16.81 MB/s, ETA ~648s\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Still uploading 1083 asset(s), remaining 10.39 GB/11.67 GB, Throughput 15.89 MB/s, ETA ~670s\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Still uploading 1077 asset(s), remaining 10.16 GB/11.64 GB, Throughput 15.54 MB/s, ETA ~670s\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Still uploading 1070 asset(s), remaining 9.89 GB/11.48 GB, Throughput 18.11 MB/s, ETA ~560s\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Still uploading 1030 asset(s), remaining 9.66 GB/11.22 GB, Throughput 16.05 MB/s, ETA ~617s\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Still uploading 986 asset(s), remaining 9.41 GB/10.82 GB, Throughput 16.43 MB/s, ETA ~587s\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Still uploading 940 asset(s), remaining 9.16 GB/10.40 GB, Throughput 17.12 MB/s, ETA ~548s\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Still uploading 898 asset(s), remaining 8.90 GB/10.03 GB, Throughput 17.32 MB/s, ETA ~527s\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Still uploading 873 asset(s), remaining 8.67 GB/9.85 GB, Throughput 15.62 MB/s, ETA ~569s\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Still uploading 866 asset(s), remaining 8.44 GB/9.80 GB, Throughput 15.57 MB/s, ETA ~556s\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Still uploading 857 asset(s), remaining 8.21 GB/9.74 GB, Throughput 15.92 MB/s, ETA ~529s\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Still uploading 837 asset(s), remaining 7.95 GB/9.54 GB, Throughput 17.71 MB/s, ETA ~460s\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Still uploading 807 asset(s), remaining 7.69 GB/9.26 GB, Throughput 17.24 MB/s, ETA ~458s\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Still uploading 747 asset(s), remaining 7.47 GB/8.83 GB, Throughput 15.29 MB/s, ETA ~501s\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Still uploading 704 asset(s), remaining 7.19 GB/8.37 GB, Throughput 18.94 MB/s, ETA ~389s\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Still uploading 673 asset(s), remaining 7.00 GB/8.17 GB, Throughput 12.87 MB/s, ETA ~557s\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Still uploading 657 asset(s), remaining 6.74 GB/8.06 GB, Throughput 17.47 MB/s, ETA ~395s\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Still uploading 653 asset(s), remaining 6.49 GB/8.00 GB, Throughput 16.87 MB/s, ETA ~394s\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Still uploading 637 asset(s), remaining 6.26 GB/7.86 GB, Throughput 15.85 MB/s, ETA ~405s\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Still uploading 620 asset(s), remaining 5.99 GB/7.69 GB, Throughput 18.03 MB/s, ETA ~341s\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Still uploading 567 asset(s), remaining 5.75 GB/7.32 GB, Throughput 16.27 MB/s, ETA ~362s\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Still uploading 519 asset(s), remaining 5.52 GB/6.84 GB, Throughput 15.25 MB/s, ETA ~371s\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Still uploading 481 asset(s), remaining 5.26 GB/6.52 GB, Throughput 17.48 MB/s, ETA ~309s\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Still uploading 452 asset(s), remaining 5.04 GB/6.24 GB, Throughput 15.32 MB/s, ETA ~337s\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Still uploading 442 asset(s), remaining 4.78 GB/6.15 GB, Throughput 17.47 MB/s, ETA ~281s\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Still uploading 432 asset(s), remaining 4.55 GB/6.07 GB, Throughput 15.49 MB/s, ETA ~301s\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Still uploading 405 asset(s), remaining 4.29 GB/5.84 GB, Throughput 17.84 MB/s, ETA ~247s\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Still uploading 370 asset(s), remaining 4.06 GB/5.56 GB, Throughput 15.10 MB/s, ETA ~276s\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Still uploading 321 asset(s), remaining 3.81 GB/5.10 GB, Throughput 17.12 MB/s, ETA ~228s\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Still uploading 288 asset(s), remaining 3.58 GB/4.82 GB, Throughput 15.27 MB/s, ETA ~241s\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Still uploading 260 asset(s), remaining 3.34 GB/4.67 GB, Throughput 16.26 MB/s, ETA ~211s\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Still uploading 230 asset(s), remaining 3.08 GB/4.39 GB, Throughput 17.94 MB/s, ETA ~176s\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Still uploading 217 asset(s), remaining 2.82 GB/4.28 GB, Throughput 17.22 MB/s, ETA ~168s\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Still uploading 203 asset(s), remaining 2.59 GB/4.19 GB, Throughput 15.81 MB/s, ETA ~168s\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Still uploading 173 asset(s), remaining 2.34 GB/3.91 GB, Throughput 16.77 MB/s, ETA ~143s\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Still uploading 132 asset(s), remaining 2.08 GB/3.48 GB, Throughput 17.62 MB/s, ETA ~121s\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Still uploading 83 asset(s), remaining 1.83 GB/3.14 GB, Throughput 16.65 MB/s, ETA ~113s\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Still uploading 61 asset(s), remaining 1.58 GB/2.86 GB, Throughput 16.83 MB/s, ETA ~97s\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Still uploading 55 asset(s), remaining 1.32 GB/2.63 GB, Throughput 17.55 MB/s, ETA ~78s\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Still uploading 52 asset(s), remaining 1.08 GB/2.49 GB, Throughput 16.39 MB/s, ETA ~68s\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Still uploading 48 asset(s), remaining 834.67 MB/2.29 GB, Throughput 18.00 MB/s, ETA ~47s\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Still uploading 33 asset(s), remaining 581.66 MB/1.64 GB, Throughput 16.68 MB/s, ETA ~35s\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Still uploading 19 asset(s), remaining 340.54 MB/1.06 GB, Throughput 15.89 MB/s, ETA ~22s\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Still uploading 14 asset(s), remaining 118.76 MB/803.56 MB, Throughput 14.61 MB/s, ETA ~9s\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Still uploading 2 asset(s), remaining 2.40 MB/114.79 MB, Throughput 7.67 MB/s, ETA ~1s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=7200, training_loss=0.06385186142391629, metrics={'train_runtime': 6551.6953, 'train_samples_per_second': 2244.335, 'train_steps_per_second': 1.099, 'total_flos': 3.3920592132368136e+16, 'train_loss': 0.06385186142391629, 'epoch': 300.0})"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ca416a15-bffc-477d-91a1-b7cc2a1a8b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"esm2_t6_8M_UR50D_300epoch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5502b128-3b9b-47ec-bd7e-0a4d32cf6e85",
   "metadata": {},
   "source": [
    "# This is an example of the ESM model trainig. For metrics calculation see tests.ipynb notebook in /main/notebooks of the current repo"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:miniconda3-ptch]",
   "language": "python",
   "name": "conda-env-miniconda3-ptch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
